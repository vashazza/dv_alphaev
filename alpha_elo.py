from __future__ import annotations
import os
import json
import re
import time
import copy
import random
import hashlib
import uuid
import logging
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Any, Tuple, Optional
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ (ë¯¸ì„¤ì¹˜/ë¯¸í‚¤ ì„¤ì • ì‹œ ëŸ°íƒ€ì„ì—ì„œë§Œ ì‹¤íŒ¨)
try:
    import anthropic
except Exception:
    anthropic = None
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# =========================
# Elo constants & helpers
# =========================
INITIAL_ELO: float = 1500.0

# ì „ì—­ ìºì‹œ: í˜ì–´ì™€ì´ì¦ˆ ì‹¬íŒ ê²°ê³¼ (A vs B ìˆœì„œ í¬í•¨)
_PAIR_CACHE: Dict[Tuple[str, str], str] = {}


def _sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()


# === numbering stripper for pretty outputs ===
_NUM_PREFIX = re.compile(r'^\s*(?:\(?\d{1,3}\)?\s*(?:[.)]|[-â€“â€”:])\s+)')
def strip_leading_numbering(s: str) -> str:
    return _NUM_PREFIX.sub('', s or '')


def _pair_cache_key(text_a: str, text_b: str, constitution: str, domain_profile: str, task_profile: str) -> Tuple[str, str]:
    # ì»¨í…ìŠ¤íŠ¸(í—Œë²•/ë„ë©”ì¸/íƒœìŠ¤í¬)ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í¬í•¨
    ctx = _sha1("||".join([constitution, domain_profile, task_profile]))
    return (_sha1("A|" + ctx + "|" + text_a), _sha1("B|" + ctx + "|" + text_b))


def _elo_expected(r_a: float, r_b: float) -> float:
    return 1.0 / (1.0 + 10 ** ((r_b - r_a) / 400.0))


def update_elo(r_a: float, r_b: float, outcome_a: float, k: float = 24.0) -> Tuple[float, float]:
    """Return new (r_a, r_b) after a single match.
    outcome_a: 1.0 (A win), 0.5 (draw), 0.0 (A loss)
    """
    e_a = _elo_expected(r_a, r_b)
    e_b = 1.0 - e_a
    r_a_new = r_a + k * (outcome_a - e_a)
    r_b_new = r_b + k * ((1.0 - outcome_a) - e_b)
    return r_a_new, r_b_new


# PATCH: ë™ì  K (ë‚œì´ë„/ê²½ê¸°ìˆ˜ ê¸°ë°˜ ì•½í•œ ìŠ¤ì¼€ì¼ë§)
def effective_k(base_k: float, r_a: float, r_b: float, games_a: int, games_b: int, min_k: float = 8.0) -> float:
    delta = abs(r_a - r_b)
    # ë¸íƒ€ê°€ ì‘ì„ìˆ˜ë¡(ëª¨ë¸ì´ ë¹„ìŠ·í• ìˆ˜ë¡) ë” í¬ê²Œ, ê²½ê¸°ìˆ˜ ë§ì„ìˆ˜ë¡ ì ì°¨ ì‘ê²Œ
    scale_delta = 1.0 / (1.0 + delta / 100.0)
    scale_games = 1.0 / (1.0 + max(games_a, games_b) / 10.0)
    k = base_k * 0.5 * (scale_delta + scale_games)
    return max(min_k, k)


@dataclass
class EvolverConfig:
    anthropic_api_key: str = ""
    openai_api_key: str = ""
    generator_model: str = "claude-sonnet-4-20250514"
    judge_model: str = "gpt-4o-mini"

    generations: int = 5
    population_per_gen: int = 8
    parallel_workers: int = 3

    judge_weights: Dict[str, int] = field(default_factory=lambda: {
        "constitution": 40,
        "domain": 30,
        "task": 30,
    })

    output_dir: str = "single_spec_result_ver1"
    use_timestamp_suffix: bool = False  # í´ë”ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ ì—¬ë¶€
    random_seed: int = 1234

    # === Task-diverse parent sampling ===
    use_task_diversity: bool = True
    task_pool_size: int = 30
    parent_selection_size: int = 10
    diverse_parent_mix: Dict[str, float] = field(default_factory=lambda: {"top": 0.8, "low": 0.2})

    # === Pairwise Elo ì„¤ì • ===
    use_pairwise_elo: bool = True
    elo_k: float = 24.0
    use_ab_ba: bool = True
    elo_initial: float = 1500.0
    elo_dynamic_k: bool = True
    elo_min_k: float = 8.0
    significance_gap: int = 30
    use_score_normalization: bool = True
    parent_selection_metric: str = "score_norm"

    # === ë¹„ìš© ì ˆê° ì„¤ì • ===
    pairwise_top_m: int = 64
    elo_neighbor_k: int = 8
    matches_floor: int = 1
    matches_ceil: int = 4


# ===== Utility IDs =====

def make_id(text: str, suffix: str = "") -> str:
    # ê³¼ê±° í˜¸í™˜ì„ ìœ„í•´ ìœ ì§€(ë‚´ìš© ê¸°ë°˜ í•´ì‹œ), ê°€ëŠ¥í•˜ë©´ make_unique_id ì‚¬ìš© ê¶Œì¥
    base_text = f"{text}{suffix}"
    return hashlib.sha1(base_text.encode('utf-8')).hexdigest()[:12]


def make_unique_id(text: str, generation: int = 0, index: int = 0) -> str:
    # ì¶©ëŒ ë°©ì§€ ë° ê°€ë…ì„±: uuid4 ì‚¬ìš©
    return uuid.uuid4().hex[:12]


def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


# =========================
# Archive (ì •ë ¬ì„ Elo ìš°ì„ )
# =========================
class Archive:
    # PATCH: elo_initial ì¸ì ì¶”ê°€
    def __init__(self, max_capacity: int = 100, low_task_reservoir_capacity: int = 30, elo_initial: float = INITIAL_ELO):
        self.max_capacity = max_capacity
        self.elo_initial = elo_initial
        self.specs: List[Dict[str, Any]] = []
        self.low_task_reservoir_capacity = low_task_reservoir_capacity
        self.low_task_reservoir: List[Dict[str, Any]] = []

    def _dedup(self, lst: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        seen, out = set(), []
        for s in lst:
            sid = s.get('id')
            if sid and sid not in seen:
                seen.add(sid)
                out.append(s)
        return out

    def _update_low_task_reservoir(self, spec: Dict[str, Any]):
        if 'scores' not in spec:
            return
        self.low_task_reservoir = [s for s in self.low_task_reservoir if s.get('id') != spec.get('id')]
        self.low_task_reservoir.append(spec)
        # ê¸°ë³¸ì€ task ì ìˆ˜ ê¸°ì¤€(ì°¸ê³ ìš©), ì •ê·œí™” ì ìˆ˜ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ ë³´ì¡°
        self.low_task_reservoir.sort(key=lambda x: x.get('scores', {}).get('task', 0))
        if len(self.low_task_reservoir) > self.low_task_reservoir_capacity:
            self.low_task_reservoir = self.low_task_reservoir[:self.low_task_reservoir_capacity]

    @staticmethod
    def _rank_key(x: Dict[str, Any]) -> Tuple[float, float]:
        # PATCH: Elo ìš°ì„ , íƒ€ì´ë¸Œë ˆì´í¬ëŠ” score_norm -> score
        score_norm = float(x.get('score_norm', x.get('score', 0.0)))
        return (float(x.get('elo', INITIAL_ELO)), score_norm)

    def _merge_stats(self, dst: Dict[str, Any], src: Dict[str, Any]):
        """ë™ì¼ ID ì—…ë°ì´íŠ¸ ì‹œ in-place ë³‘í•© (ê²½ê¸°ìˆ˜/ì ìˆ˜ ìœ ì‹¤ ë°©ì§€)."""
        stat_keys = [
            'elo', 'games', 'wins', 'losses', 'draws',
            'score', 'score_norm', 'scores', 'scores_norm', 'score_weighted',
            'provenance', 'evaluated_at', 'text', 'meta'
        ]
        for k in stat_keys:
            if k in src:
                dst[k] = src[k]

    def add(self, spec: Dict[str, Any]):
        # Elo í•„ë“œ ê¸°ë³¸ê°’ ë³´ì¥ (cfg ê¸°ë°˜ ì´ˆê¸°ê°’)
        spec.setdefault('elo', self.elo_initial)
        spec.setdefault('games', 0)
        spec.setdefault('wins', 0)
        spec.setdefault('losses', 0)
        spec.setdefault('draws', 0)

        existing_ids = {s['id'] for s in self.specs}
        if spec['id'] in existing_ids:
            for i, s in enumerate(self.specs):
                if s['id'] == spec['id']:
                    # PATCH: ìˆœìœ„ ìš°ì—´ê³¼ ë¬´ê´€í•˜ê²Œ ìƒíƒœ in-place ë³‘í•©
                    self._merge_stats(s, spec)
                    self.specs.sort(key=self._rank_key, reverse=True)
                    self._update_low_task_reservoir(s)
                    return

        self.specs.append(spec)
        self.specs.sort(key=self._rank_key, reverse=True)
        if len(self.specs) > self.max_capacity:
            self.specs = self.specs[:self.max_capacity]
        self._update_low_task_reservoir(spec)

    def sample_parents(self, n: int) -> List[Dict[str, Any]]:
        if not self.specs:
            return []
        top_30_specs = min(30, len(self.specs))
        candidates = self.specs[:top_30_specs]
        return random.sample(candidates, min(n, len(candidates)))

    # PATCH: ë¶€ëª¨ ìƒ˜í”Œë§ì— ì‚¬ìš©í•  ë©”íŠ¸ë¦­ì„ ì™¸ë¶€ì—ì„œ ì§€ì • ê°€ëŠ¥
    def sample_parents_task_diverse(self, n: int, mix: Dict[str, float], task_pool_size: int,
                                    metric: str = "task") -> List[Dict[str, Any]]:
        if not self.specs:
            return []
        total = max(1, n)
        top_ratio = max(0.0, min(1.0, mix.get("top", 0.8)))
        n_top = max(1, int(round(total * top_ratio)))
        n_low = total - n_top

        def metric_value(s: Dict[str, Any]) -> float:
            if metric == "score_norm":
                return float(s.get('score_norm', 0.0))
            elif metric == "score":
                return float(s.get('score', 0.0))
            else:
                return float(s.get('scores', {}).get('task', 0.0))

        by_metric_desc = sorted(self.specs, key=metric_value, reverse=True)
        by_metric_asc = list(reversed(by_metric_desc))
        top_pool = by_metric_desc[:min(task_pool_size, len(by_metric_desc))]
        low_pool_main = by_metric_asc[:min(task_pool_size, len(by_metric_asc))]
        low_pool = self._dedup(low_pool_main + self.low_task_reservoir)

        sel: List[Dict[str, Any]] = []
        if top_pool and n_top > 0:
            chosen_top = random.sample(top_pool, min(n_top, len(top_pool)))
            sel += [dict(x, **{"_tier": "top"}) for x in chosen_top]  # âœ… ë¼ë²¨ë§
        if low_pool and n_low > 0:
            chosen_low = random.sample(low_pool, min(n_low, len(low_pool)))
            sel += [dict(x, **{"_tier": "low"}) for x in chosen_low]  # âœ… ë¼ë²¨ë§

        sel = self._dedup(sel)
        if len(sel) < total:
            fallback = [s for s in by_metric_desc if s.get('id') not in {x.get('id') for x in sel}]
            # ë¶€ì¡±ë¶„ì€ ê¸°ë³¸ì ìœ¼ë¡œ TOPì—ì„œ ì±„ìš°ë˜ ë¼ë²¨ ë¶€ì—¬
            fallback = [dict(x, **{"_tier": "top"}) for x in fallback]
            sel += fallback[:total - len(sel)]
        return sel[:total]
    

    def all_elites(self) -> List[Dict[str, Any]]:
        return self.specs.copy()


# =========================
# í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜ë“¤
# =========================

def load_text_prompt(file_path: str) -> str:
    """í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    # ì¡°ìš©íˆ ë¡œë“œ (verbose ë©”ì‹œì§€ ì œê±°)
    if not os.path.exists(file_path):
        return ""

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            return content
    except Exception as e:
        return ""

def load_json_prompt(file_path: str) -> List[Dict[str, Any]]:
    """JSON í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ (ì¡°ìš©íˆ)"""
    if not os.path.exists(file_path):
        return []

    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)



# =========================
# í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € - í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì¬ì‚¬ìš©
# =========================
class PromptManager:
    def __init__(self):
        self.generator_prompt = None
        self.pairwise_referee_prompt = None
        self.improvement_approaches = None
        self._load_all_prompts()
    
    def _load_all_prompts(self):
        """ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ë¡œë“œ (ì¡°ìš©íˆ)"""
        # Generator í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.generator_prompt = load_text_prompt("prompts/generator_prompt.txt")
        if not self.generator_prompt:
            raise FileNotFoundError("Generator í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompts/generator_prompt.txt íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # Pairwise Referee í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        self.pairwise_referee_prompt = load_text_prompt("prompts/pairwise_referee_prompt.txt")
        if not self.pairwise_referee_prompt:
            raise FileNotFoundError("Pairwise Referee í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompts/pairwise_referee_prompt.txt íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # Improvement Approaches ë¡œë“œ
        self.improvement_approaches = load_json_prompt("prompts/improvement_approaches.json")
        if not self.improvement_approaches:
            raise FileNotFoundError("improvement_approaches.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompts/improvement_approaches.json íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì „ì—­ í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_prompt_manager = None

def get_prompt_manager() -> PromptManager:
    """í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _prompt_manager
    if _prompt_manager is None:
        _prompt_manager = PromptManager()
    return _prompt_manager

# =========================
# Model wrappers
# =========================
def call_with_retry(fn, *, tries: int = 3, backoff: float = 0.8):
    for i in range(tries):
        try:
            return fn()
        except Exception as e:
            if i == tries - 1:
                raise
            time.sleep(backoff * (2 ** i))


class AnthropicClientWrapper:
    def __init__(self, api_key: str, model: str = None):
        if anthropic is None:
            raise ImportError("anthropic package not available")
        self.client = anthropic.Anthropic(api_key=api_key)
        self.model = model

    def generate(self, prompt: str, max_tokens: int = 1200, temperature: float = 0.3) -> str:
        def _call():
            resp = self.client.messages.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            try:
                return resp.content[0].text
            except Exception:
                try:
                    return getattr(resp, 'completion', '')
                except Exception:
                    return str(resp)
        return call_with_retry(_call)


class OpenAIClientWrapper:
    def __init__(self, api_key: str, model: str = None):
        if OpenAI is None:
            raise ImportError("openai package not available")
        self.client = OpenAI(api_key=api_key)
        self.model = model

    def generate(self, prompt: str, max_tokens: int = 1200, temperature: float = 0.3) -> str:
        def _call():
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            try:
                return resp.choices[0].message.content
            except Exception:
                return str(resp)
        return call_with_retry(_call)


# =========================
# Generation (multi-parent refine)
# =========================
def llm_refine_multi_parent(
    parent_specs: List[Dict[str, Any]],
    generator,
    constitution: str,
    domain_profile: str,
    task_profile: str,
    generation: int = 0,
    generator_log_dir: str = None,
    domain_name: str = None,
    task_name: str = None,
    domain_concepts: List[str] = None,
    task_concepts: List[str] = None
) -> Dict[str, Any]:
    """ì—¬ëŸ¬ ë¶€ëª¨ specë“¤ì„ ì¡°í•©í•´ì„œ ê°œì„ ëœ spec ìƒì„± (TOP/LOW êµ¬ë¶„ ë°˜ì˜)"""
    # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ê°œì„  ë°©í–¥ë“¤ ê°€ì ¸ì˜¤ê¸°
    pm = get_prompt_manager()
    selected_approach = random.choice(pm.improvement_approaches)

    # --- TOP/LOW ë¼ë²¨ ì •ê·œí™” (ê¸°ë³¸: TOP) ---
    norm_parent_specs = []
    for p in parent_specs[:10]:
        tier = p.get('_tier', 'top')
        if tier not in ('top', 'low'):
            tier = 'top'
        q = dict(p)  # ì–•ì€ ë³µì‚¬: ì›ë³¸ ì˜¤ì—¼ ë°©ì§€
        q['_tier'] = tier
        norm_parent_specs.append(q)

    tops = [p for p in norm_parent_specs if p['_tier'] == 'top']
    lows = [p for p in norm_parent_specs if p['_tier'] == 'low']

    def _fmt_examples(lst: List[Dict[str, Any]], tag: str) -> str:
        out = ""
        for i, parent in enumerate(lst, 1):
            score = parent.get('score', 0)
            elo = parent.get('elo', INITIAL_ELO)
            text = parent.get('text', '')
            out += f"\n[{tag}] Example {i} (Score: {score}/100, Elo: {elo:.1f}):\n{text}\n"
        return out if out else f"\n[{tag}] (none)\n"

    # NEW: í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë³€ìˆ˜ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜
    approach_focus = selected_approach['focus']
    approach_description = selected_approach['description']
    good_examples = _fmt_examples(tops, "TOP")
    bad_examples = _fmt_examples(lows, "LOW")
    approach_focus_lower = approach_focus.lower()

    # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ë¯¸ë¦¬ ë¡œë“œëœ í…œí”Œë¦¿ ì‚¬ìš©
    prompt_template = pm.generator_prompt
    
    # ë™ì  ë³€ìˆ˜ë“¤ ì¤€ë¹„
    task_type = task_name or "general task"
    domain_type = domain_name or "general domain"
    task_concepts_str = ", ".join(task_concepts or []) or "general task concepts"
    domain_concepts_str = ", ".join(domain_concepts or []) or "general domain concepts"
    
    prompt = prompt_template.format(
        domain_profile=domain_profile,
        task_profile=task_profile,
        constitution=constitution,
        approach_focus=approach_focus,
        approach_description=approach_description,
        approach_focus_lower=approach_focus_lower,
        good_examples=good_examples,
        bad_examples=bad_examples,
        task_type=task_type,
        domain_type=domain_type,
        task_concepts=task_concepts_str,
        domain_concepts=domain_concepts_str
    )

    try:
        refined = generator.generate(prompt, max_tokens=2000, temperature=0.4)
        if generator_log_dir:
            timestamp = int(time.time() * 1000)
            log_file = os.path.join(generator_log_dir, f"gen{generation:03d}_multi_parent_{timestamp}.json")
            log_data = {
                'generation': generation,
                'timestamp': timestamp,
                'type': 'multi_parent_refinement',
                'parent_count': len(norm_parent_specs),
                'parent_ids': [p.get('id') for p in norm_parent_specs],
                'parent_tiers': [p.get('_tier') for p in norm_parent_specs],
                'prompt': prompt,
                'response': refined,
                'temperature': 0.4,
                'max_tokens': 2000
            }
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ğŸš¨ Generator í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        print(f"ğŸ”§ Fallbackìœ¼ë¡œ ì²« ë²ˆì§¸ ë¶€ëª¨ spec ì‚¬ìš©")
        fallback_spec = copy.deepcopy(norm_parent_specs[0]) if norm_parent_specs else {'text': '// LLM-refine-failed: ' + str(e)}
        fallback_spec['id'] = make_unique_id(fallback_spec.get('text', 'fallback'))
        return fallback_spec

    new_spec = {
        'text': refined,
        'id': make_unique_id(refined, generation, 0),
        'meta': {
            'origin': 'multi_parent_refinement',
            'parent_count': len(norm_parent_specs),
            'parent_tiers': [p.get('_tier') for p in norm_parent_specs]
        }
    }
    return new_spec

# =========================
# Split LLM output â†’ individual specs
# =========================

def split_llm_response_to_specs(llm_response: str, parent_id: str) -> List[Dict[str, Any]]:
    import re
    lines = [line.strip() for line in llm_response.splitlines() if line.strip()]
    rfc2119_keywords = [
        'MUST', 'MUST NOT', 'MUST_NOT',
        'SHOULD', 'SHOULD NOT', 'SHOULD_NOT',
        'SHALL', 'SHALL NOT', 'SHALL_NOT',
        'MAY', 'REQUIRED', 'RECOMMENDED', 'OPTIONAL'
    ]

    # ë³´ë‹¤ ê°•ê±´í•œ í—¤ë” ë§¤ì¹­ (ë¶ˆë¦¿/ì½œë¡ /ëŒ€ì†Œë¬¸ì í—ˆìš©)
    KW = r"(MUST(?:\s+NOT)?|SHOULD(?:\s+NOT)?|SHALL(?:\s+NOT)?|MAY|REQUIRED|RECOMMENDED|OPTIONAL)"
    BUL = r"(?:\d+\.\s*|[-*â€¢â€“â€”]\s*)"
    import re as _re
    pat_head = _re.compile(rf"^\s*(?:{BUL})?\s*{KW}\b[:\-]?\s*(.*)", _re.I)

    spec_lines: List[str] = []
    current_spec = ""

    for line in lines:
        m = pat_head.match(line)
        if m:
            # ìƒˆ ìŠ¤í™ ì‹œì‘
            content = m.group(0).strip()
            if current_spec:
                spec_lines.append(current_spec.strip())
            current_spec = content
        else:
            # ì´ì–´ì§€ëŠ” ì¤„ í•©ì¹˜ê¸° (ë¶ˆë¦¿ ì œê±°)
            cleaned = _re.sub(r'^\s*(?:\d+\.\s*|[-*â€¢â€“â€”]\s*)', '', line).strip()
            if current_spec:
                current_spec += " " + cleaned

    if current_spec:
        spec_lines.append(current_spec.strip())
    print(f"    ğŸ” í•„í„°ë§ ê²°ê³¼: {len(lines)}ì¤„ â†’ {len(spec_lines)}ê°œ RFC2119 spec")

    individual_specs: List[Dict[str, Any]] = []
    for i, spec_text in enumerate(spec_lines):
        spec = {
            'id': make_unique_id(spec_text, generation=0, index=i),
            'text': strip_leading_numbering(spec_text),
            'meta': {'origin': 'llm_split', 'index': i, 'parent_id': parent_id},
            'provenance': [{'op': 'llm_refine_split', 'parent': parent_id}],
            'elo': INITIAL_ELO,
            'games': 0, 'wins': 0, 'losses': 0, 'draws': 0,
        }
        individual_specs.append(spec)
    return individual_specs


# =========================
# Variation wrapper
# =========================

def apply_variation_multi_parent(parent_specs: List[Dict[str, Any]], generator, constitution: str, domain_profile: str, task_profile: str, generation: int = 0, generator_log_dir: str = None, domain_name: str = None, task_name: str = None, domain_concepts: List[str] = None, task_concepts: List[str] = None) -> List[Dict[str, Any]]:
    refined_response = llm_refine_multi_parent(parent_specs, generator, constitution, domain_profile, task_profile, generation, generator_log_dir, domain_name, task_name, domain_concepts, task_concepts)
    individual_specs = split_llm_response_to_specs(refined_response['text'], 'multi_parent')
    parent_ids = [p['id'] for p in parent_specs[:5]]
    parent_tiers = refined_response.get('meta', {}).get('parent_tiers', [])
    for spec in individual_specs:
        spec.setdefault('provenance', []).append({
            'op': 'multi_parent_refine_split',
            'parent_count': len(parent_specs),
            'parent_ids': parent_ids,
            'parent_tiers': parent_tiers  # âœ… TOP/LOW ë¼ë²¨ ë³´ì¡´
        })
    if not individual_specs:
        fallback = copy.deepcopy(parent_specs[0]) if parent_specs else {'text': '// Multi-parent failed', 'id': make_unique_id('fallback')}
        fallback.setdefault('provenance', []).append({'op': 'multi_parent_fallback'})
        fallback.setdefault('elo', INITIAL_ELO)
        fallback.setdefault('games', 0)
        fallback.setdefault('wins', 0)
        fallback.setdefault('losses', 0)
        fallback.setdefault('draws', 0)
        return [fallback]
    return individual_specs


# =========================
# Judges (Pointwise) & Pairwise Referee
# =========================
class Judge:
    def __init__(self, name: str, client):
        self.name = name
        self.client = client

        # í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œí•˜ê³  ì €ì¥ (ì¡°ìš©íˆ)
        if self.name == "domain":
            self.prompt_template = load_text_prompt("prompts/domain_judge_prompt.txt")
        elif self.name == "task":
            self.prompt_template = load_text_prompt("prompts/task_judge_prompt.txt")
        else:  # constitution
            self.prompt_template = load_text_prompt("prompts/constitution_judge_prompt.txt")

        if not self.prompt_template:
            raise FileNotFoundError(f"{self.name} Judge í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompts/{self.name}_judge_prompt.txt íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    def score(self, spec_text: str, constitution: str, domain_profile: str, task_profile: str, max_tokens: int = 300) -> Tuple[float, str, str]:
        # ì €ì¥ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
        if not hasattr(self, 'prompt_template') or not self.prompt_template:
            print(f"ğŸš¨ {self.name} Judge: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            raise RuntimeError(f"{self.name} Judge: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
        if self.name == "domain":
            prompt = self.prompt_template.format(
                domain_profile=domain_profile,
                spec_text=spec_text
            )
        elif self.name == "task":
            prompt = self.prompt_template.format(
                task_profile=task_profile,
                spec_text=spec_text
            )
        else:  # constitution
            prompt = self.prompt_template.format(
                constitution=constitution,
                spec_text=spec_text
            )
        try:
            raw = self.client.generate(prompt, max_tokens=max_tokens, temperature=0.1)
            if self.name in ("domain", "task"):
                total = None
                for line in raw.split('\n'):
                    if line.startswith('TOTAL='):
                        try:
                            total = int(line.split('=')[1].strip())
                            break
                        except:
                            pass
                if total is None:
                    return 0, prompt, raw
                return max(0, min(30, total)), prompt, raw
            elif self.name == "constitution":
                total = None
                for line in raw.split('\n'):
                    if line.startswith('TOTAL='):
                        try:
                            total = int(line.split('=')[1].strip())
                            break
                        except:
                            pass
                if total is None:
                    return 0, prompt, raw
                return max(0, min(40, total)), prompt, raw
            else:
                return 0.0, prompt, raw
        except Exception:
            return 0.0, "", ""


def pairwise_referee_decision(client, text_a: str, text_b: str, constitution: str, domain_profile: str, task_profile: str, max_tokens: int = 200) -> str:
    """ìŠ¹/íŒ¨/ë¬´ë§Œ íŒë‹¨. Returns 'A', 'B', or 'TIE'"""
    # í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ë¯¸ë¦¬ ë¡œë“œëœ í…œí”Œë¦¿ ì‚¬ìš©
    pm = get_prompt_manager()
    prompt_template = pm.pairwise_referee_prompt
    
    prompt = prompt_template.format(
        constitution=constitution,
        domain_profile=domain_profile,
        task_profile=task_profile,
        text_a=text_a,
        text_b=text_b
    )
    try:
        raw = client.generate(prompt, max_tokens=max_tokens, temperature=0.0)
    except Exception:
        return 'TIE'
    ans = 'TIE'
    for line in reversed(raw.splitlines()):
        ls = line.strip().upper()
        if ls.startswith('ANSWER:'):
            token = ls.split(':', 1)[1].strip()
            if token in ('A', 'B', 'TIE'):
                ans = token
            break
    return ans


def pairwise_referee_decision_cached(client, text_a: str, text_b: str, constitution: str, domain_profile: str, task_profile: str, max_tokens: int = 200) -> str:
    """ìºì‹œ ì‚¬ìš© ë²„ì „"""
    k = _pair_cache_key(text_a, text_b, constitution, domain_profile, task_profile)
    if k in _PAIR_CACHE:
        return _PAIR_CACHE[k]
    ans = pairwise_referee_decision(client, text_a, text_b, constitution, domain_profile, task_profile, max_tokens=max_tokens)
    _PAIR_CACHE[k] = ans
    return ans


def pairwise_ab_ba_conditional(client, text_a: str, text_b: str, constitution: str, domain_profile: str, task_profile: str, use_ab_ba: bool = True) -> float:
    """AB/BA ì—­ì „ìœ¼ë¡œ ìœ„ì¹˜ í¸í–¥ ìƒì‡„. ë°˜í™˜ê°’ì€ A ê´€ì  outcome(1/0.5/0), ì²« ê²°ê³¼ê°€ TIEë©´ BA ìƒëµ."""
    res_ab = pairwise_referee_decision_cached(client, text_a, text_b, constitution, domain_profile, task_profile)
    if (not use_ab_ba) or (res_ab == 'TIE'):
        return 1.0 if res_ab == 'A' else (0.0 if res_ab == 'B' else 0.5)
    res_ba = pairwise_referee_decision_cached(client, text_b, text_a, constitution, domain_profile, task_profile)
    res_ba_mapped = 'B' if res_ba == 'A' else ('A' if res_ba == 'B' else 'TIE')
    if res_ab == res_ba_mapped:
        return 1.0 if res_ab == 'A' else (0.0 if res_ab == 'B' else 0.5)
    else:
        return 0.5


# =========================
# Pointwise evaluation (ì°¸ê³ ìš© ì ìˆ˜)
# =========================

def evaluate_spec_with_judges(spec: Dict[str, Any], judges: Dict[str, Judge], weights: Dict[str, int],
                              constitution: str, domain_profile: str, task_profile: str, 
                              generation: int = 0, quality_threshold: int = 3, judges_log_dir: str = None, is_top10: bool = False,
                              initial_elo: float = INITIAL_ELO) -> Dict[str, Any]:
    scores = {'constitution': 0, 'domain': 0, 'task': 0}
    spec_text = spec.get('text', '').strip()
    if not spec_text:
        out = copy.deepcopy(spec)
        out['scores'] = scores
        out['score'] = 0
        out['score_weighted'] = 0.0
        out['evaluated_at'] = time.time()
        out.setdefault('elo', initial_elo)
        out.setdefault('games', 0)
        out.setdefault('wins', 0)
        out.setdefault('losses', 0)
        out.setdefault('draws', 0)
        return out

    judge_responses = {}
    for k, j in judges.items():
        try:
            sc, prompt, raw_response = j.score(spec_text, constitution, domain_profile, task_profile)
            scores[k] = sc
            if is_top10 and judges_log_dir:
                judge_responses[k] = {
                    'judge_type': k,
                    'prompt': prompt,
                    'raw_response': raw_response,
                    'parsed_score': sc,
                    'temperature': 0.1,
                    'max_tokens': 300
                }
        except Exception:
            scores[k] = 0

    if is_top10 and judges_log_dir and len(judge_responses) > 0:
        timestamp = int(time.time() * 1000)
        log_file = os.path.join(judges_log_dir, f"gen{generation:03d}_{spec.get('id', 'unknown')}_all_judges.json")
        log_data = {
            'generation': generation,
            'timestamp': timestamp,
            'spec_id': spec.get('id', 'unknown'),
            'spec_text': spec_text[:500],
            'judges': judge_responses,
            'final_scores': scores,
            'total_score': 0
        }
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Warning: Judge ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨ {log_file}: {e}")

    constitution_score = scores.get('constitution', 0)
    domain_score = scores.get('domain', 0)
    task_score = scores.get('task', 0)
    total = constitution_score + domain_score + task_score

    # ê°€ì¤‘ í•©ì‚°(ì‹¤ì œ weights ë°˜ì˜)
    w = weights or {"constitution": 40, "domain": 30, "task": 30}
    score_weighted = (
        (constitution_score / 40.0) * w.get('constitution', 40) +
        (domain_score / 30.0) * w.get('domain', 30) +
        (task_score / 30.0) * w.get('task', 30)
    )

    if is_top10 and judges_log_dir and len(judge_responses) > 0:
        log_file = os.path.join(judges_log_dir, f"gen{generation:03d}_{spec.get('id', 'unknown')}_all_judges.json")
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    log_data = json.load(f)
                log_data['total_score'] = total
                log_data['score_weighted'] = score_weighted
                with open(log_file, 'w', encoding='utf-8') as f:
                    json.dump(log_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"Warning: total_score ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ {log_file}: {e}")

    out = copy.deepcopy(spec)
    out['scores'] = scores
    out['score'] = total
    out['score_weighted'] = score_weighted
    out['evaluated_at'] = time.time()
    out.setdefault('elo', initial_elo)
    out.setdefault('games', 0)
    out.setdefault('wins', 0)
    out.setdefault('losses', 0)
    out.setdefault('draws', 0)
    return out


# =========================
# Score normalization (í¸í–¥ ì™„í™”)
# =========================
def normalize_judge_scores_for_pool(specs: List[Dict[str, Any]]):
    import statistics as st
    keys = ['constitution', 'domain', 'task']
    stats = {}
    for k in keys:
        vals = [float(s.get('scores', {}).get(k, 0.0)) for s in specs]
        mu = st.mean(vals) if vals else 0.0
        sd = st.pstdev(vals) if len(vals) > 1 else 1.0
        stats[k] = (mu, sd or 1.0)
    for s in specs:
        s.setdefault('scores_norm', {})
        for k in keys:
            mu, sd = stats[k]
            s['scores_norm'][k] = (float(s.get('scores', {}).get(k, 0.0)) - mu) / sd
        # ê°€ì¤‘ í•©: constitution 0.4, domain 0.3, task 0.3
        s['score_norm'] = (
            s['scores_norm']['constitution'] * 0.4 +
            s['scores_norm']['domain'] * 0.3 +
            s['scores_norm']['task'] * 0.3
        )


# === PII masking + embedding-based (or shingle) deduplication ===
PII_PATTERNS = [
    re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}"),
    re.compile(r"\+?\d[\d\s\-()]{7,}\d"),
    re.compile(r"\b\d{2,}[-/\s]?\d{2,}[-/\s]?\d{2,}\b"),
]
def mask_pii(text: str) -> str:
    t = text or ""
    for p in PII_PATTERNS:
        t = p.sub("[REDACTED]", t)
    t = re.sub(r"\d{4,}", "[NUMBER]", t)
    return t

def _l2_normalize(M):
    import numpy as _np
    M = _np.asarray(M, dtype=float)
    norms = _np.linalg.norm(M, axis=1, keepdims=True) + 1e-12
    return M / norms

_EMBED_BACKEND = None

def extract_semantic_core_generic(text):
    """RFC2119 í‚¤ì›Œë“œì™€ ì¼ë°˜ì ì¸ êµ¬ì¡°ì–´ë§Œ ì œê±° (ë„ë©”ì¸ ë¬´ê´€)"""
    import re
    
    # 1. RFC2119 í‚¤ì›Œë“œ ì œê±°
    text = re.sub(r'^\s*(MUST|SHOULD|SHALL|MAY|REQUIRED|RECOMMENDED|OPTIONAL)(\s+NOT)?\s+', '', text, flags=re.IGNORECASE)
    
    # 2. ë§¤ìš° ì¼ë°˜ì ì¸ êµ¬ì¡°ì–´ë§Œ ì œê±° (ë„ë©”ì¸ íŠ¹í™” ë‹¨ì–´ëŠ” ëª¨ë‘ ë³´ì¡´)
    generic_stop_words = {
        'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
        'when', 'where', 'how', 'that', 'this', 'these', 'those', 'all', 'any', 'each', 'every'
    }
    
    # 3. ë‹¨ì–´ ë¶„ë¦¬ í›„ ë§¤ìš° ê¸°ë³¸ì ì¸ ë¶ˆìš©ì–´ë§Œ ì œê±°
    words = []
    for word in text.split():
        clean_word = re.sub(r'[^\w\-]', '', word).lower()  # êµ¬ë‘ì  ì œê±°
        if clean_word and clean_word not in generic_stop_words and len(clean_word) > 1:
            words.append(clean_word)
    
    return ' '.join(words)

# ì˜ë¯¸ì  ê°œë… ì¶”ì¶œ í•¨ìˆ˜ë“¤ ì œê±°ë¨ - ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ì¶”ì¶œë§Œ ì‚¬ìš©

# =========================
# ê°œì„ ëœ í´ëŸ¬ìŠ¤í„°ë§ ì‹œìŠ¤í…œ
# =========================

# í´ëŸ¬ìŠ¤í„°ë§ í•¨ìˆ˜ë“¤ ì œê±°ë¨ - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì§€ë°° ê´€ê³„ ì¤‘ë³µ ì œê±°ë¡œ ëŒ€ì²´

def keyword_enrich(texts, topk=None):
    """Lightweight TF-IDF keyword boosting - ë„ë©”ì¸ ë¬´ê´€í•œ ê¸°ë³¸ ë¶ˆìš©ì–´ë§Œ ì‚¬ìš©
    - RFC2119ì™€ ë§¤ìš° ì¼ë°˜ì ì¸ ë‹¨ì–´ë§Œ ì œê±°í•˜ì—¬ ë„ë©”ì¸ ë…ë¦½ì„± ë³´ì¥
    - ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œëŠ” ëª¨ë‘ ë³´ì¡´
    """
    from sklearn.feature_extraction.text import TfidfVectorizer
    import numpy as np
    import re

    if not texts:
        return texts

    # ë§¤ìš° ê¸°ë³¸ì ì¸ ë¶ˆìš©ì–´ë§Œ (RFC2119 + ì¼ë°˜ êµ¬ì¡°ì–´)
    stop = {
        "must","should","shall","may","not","never","always",
        "mustn","shouldn","shan","can","cannot","can't","cant",
        "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by",
        "is", "are", "was", "were", "be", "been", "being", "have", "has", "had",
        "this", "that", "these", "those", "when", "where", "how", "what", "why", "who", "which"
    }

    try:
        vec = TfidfVectorizer(
            analyzer="word",
            ngram_range=(1, 2),
            lowercase=True,
            token_pattern=r"(?u)\b[A-Za-z][A-Za-z\-]+\b",
            stop_words=list(stop),
            min_df=2,
            max_df=0.7,  # ë” ê´€ëŒ€í•˜ê²Œ ì¡°ì •
            max_features=8000,
            sublinear_tf=True,
            norm="l2",
            smooth_idf=True,
        )
        X = vec.fit_transform(texts)
        vocab = np.array(vec.get_feature_names_out())

        # Fall back if vocabulary collapsed
        if X.shape[1] == 0:
            return texts

        A = X.toarray()
        enriched = []
        for i, t in enumerate(texts):
            # dynamic top-k: ~1 per 8 words, clamp [2,5] (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
            if topk is None:
                wc = max(1, len(re.findall(r"[A-Za-z]+", t)))
                import numpy as _np
                k = int(_np.clip(round(wc / 8), 2, 5))
            else:
                k = topk
            # pick top-k unique tokens
            idx = A[i].argsort()[-k:]
            toks = [vocab[j] for j in idx if A[i, j] > 0]
            # dedupe and keep order by score
            toks_scored = sorted(((tok, A[i, j]) for tok, j in zip(toks, idx[::-1])), key=lambda x: -x[1])
            seen = set()
            kws = []
            for tok, _ in toks_scored:
                if tok not in seen:
                    seen.add(tok)
                    kws.append(tok)
                if len(kws) >= k:
                    break
            # í‚¤ì›Œë“œ ê°•ì¡° (ì„ë² ë”© í’ˆì§ˆ í–¥ìƒìš©)
            if kws:
                enriched.append(t + "\n\n### KEY: " + ", ".join(f"[{kw}]" for kw in kws))
            else:
                enriched.append(t)
        return enriched
    except Exception as e:
        # TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (ì˜ˆì™¸ ë°œìƒ ë°©ì§€)
        print(f"âš ï¸  í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
        return texts


def embed_texts(texts, enrich=False, model_name="paraphrase-mpnet-base-v2"):
    """Return L2-normalized embeddings for texts.
    Priority:
      1) sentence-transformers 'paraphrase-mpnet-base-v2' (ë” ì •í™•í•œ ì˜ë¯¸ ìœ ì‚¬ì„±)
      2) Fallback: 'all-MiniLM-L6-v2' (ì†ë„ ìš°ì„ )
    Optionally enrich texts with lightweight TF-IDF keywords to better separate near-duplicates.
    """
    import numpy as _np
    from sklearn.preprocessing import normalize as _normalize
    
    global _EMBED_BACKEND
    
    if not texts:
        return _np.zeros((0, 16), dtype=_np.float32)

    # Optional enrichment to emphasize discriminative terms
    proc = keyword_enrich(texts) if enrich else texts

    # sentence-transformers ì‚¬ìš© (ê°œì„ ëœ ëª¨ë¸ ìš°ì„ )
    if _EMBED_BACKEND is None:
        from sentence_transformers import SentenceTransformer
        try:
            _EMBED_BACKEND = SentenceTransformer(model_name)
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}")
        except Exception as e:
            print(f"âš ï¸  {model_name} ë¡œë“œ ì‹¤íŒ¨, fallback to all-MiniLM-L6-v2: {e}")
            _EMBED_BACKEND = SentenceTransformer("all-MiniLM-L6-v2")
    
    X = _EMBED_BACKEND.encode(proc, show_progress_bar=False, normalize_embeddings=True)
    return X

def priority_hierarchical_dedup(specs, score_key='elo', keep_ratio=0.4, similarity_threshold=0.70):
    """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì§€ë°° ê´€ê³„ ì¤‘ë³µ ì œê±° (AutoPolicy ë°©ì‹ ì°¸ì¡°)"""
    if not specs or len(specs) <= 3:
        return specs
    
    import numpy as np
    
    print(f"  ğŸ¯ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ì‹œì‘: {len(specs)}ê°œ spec ë¶„ì„...")
    
    # 1. í’ˆì§ˆ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒë¶€í„°) - ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
    sorted_specs = sorted(specs, key=lambda s: float(s.get(score_key, 0)), reverse=True)
    
    # ë””ë²„ê¹…: ìƒìœ„ 5ê°œ specì˜ ì ìˆ˜ ë¶„í¬ í™•ì¸
    print(f"     ğŸ“Š ìƒìœ„ 5ê°œ ì ìˆ˜ ë¶„í¬:")
    for i in range(min(5, len(sorted_specs))):
        s = sorted_specs[i]
        elo = s.get('elo', 0)
        judge = s.get('score', 0) 
        norm = s.get('score_norm', 'N/A')
        print(f"       #{i}: Elo={elo:.1f}, Judge={judge:.1f}, Norm={norm}")
    
    # ìœ ì‚¬ë„ ë° í’ˆì§ˆ ì°¨ì´ í†µê³„
    similar_pairs = 0
    quality_diff_pairs = 0
    
    # 2. ëª¨ë“  í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± (í•œ ë²ˆë§Œ)
    texts = [mask_pii(s.get('text', '')) for s in sorted_specs]
    embeddings = embed_texts(texts)
    
    # 3. ì§€ë°° ê´€ê³„ ë¶„ì„ ë° ì¤‘ë³µ ì œê±°
    dominated = set()
    dominance_relations = []
    
    for i, spec_a in enumerate(sorted_specs):
        if i in dominated:  # ì´ë¯¸ ì§€ë°°ë‹¹í•œ ê²ƒì€ ê±´ë„ˆë›°ê¸°
            continue
            
        for j, spec_b in enumerate(sorted_specs[i+1:], i+1):
            if j in dominated:  # ì´ë¯¸ ì§€ë°°ë‹¹í•œ ê²ƒì€ ê±´ë„ˆë›°ê¸°
                continue
                
            # ìœ ì‚¬ë„ ì²´í¬
            similarity = np.dot(embeddings[i], embeddings[j])
            if similarity >= similarity_threshold:
                similar_pairs += 1
                
                # í’ˆì§ˆ ì°¨ì´ ì²´í¬
                elo_a, elo_b = spec_a.get('elo', 0), spec_b.get('elo', 0)
                if 'score_norm' in spec_a and 'score_norm' in spec_b:
                    qual_a, qual_b = spec_a.get('score_norm', 0), spec_b.get('score_norm', 0)
                elif abs(elo_a - elo_b) < 10:
                    qual_a, qual_b = spec_a.get('score', 0), spec_b.get('score', 0)
                else:
                    qual_a, qual_b = elo_a, elo_b
                
                if qual_a > qual_b:
                    quality_diff_pairs += 1
                
            # ì§€ë°° ê´€ê³„ íŒë‹¨
            is_dominant, dominance_score = calculate_dominance_relationship(
                spec_a, spec_b, embeddings[i], embeddings[j], similarity_threshold
            )
            
            if is_dominant:
                dominated.add(j)
                dominance_relations.append((i, j, dominance_score))
    
    # ë””ë²„ê¹… í†µê³„ ì¶œë ¥
    total_pairs = len(sorted_specs) * (len(sorted_specs) - 1) // 2
    print(f"     ğŸ“Š ë¶„ì„ í†µê³„: ì´ {total_pairs}ìŒ ì¤‘")
    print(f"       - ìœ ì‚¬ë„ {similarity_threshold:.2f} ì´ìƒ: {similar_pairs}ìŒ ({similar_pairs/total_pairs*100:.1f}%)")
    print(f"       - í’ˆì§ˆ ì°¨ì´ ìˆìŒ: {quality_diff_pairs}ìŒ ({quality_diff_pairs/max(similar_pairs,1)*100:.1f}%)")
    print(f"       - ì§€ë°° ê´€ê³„ ì„±ë¦½: {len(dominance_relations)}ìŒ")
    
    # 4. ì§€ë°°ë‹¹í•˜ì§€ ì•Šì€ specë“¤ ì„ íƒ
    survivors = [spec for i, spec in enumerate(sorted_specs) if i not in dominated]
    
    # 5. ì§€ë°° ê´€ê³„ ì¶œë ¥ (ì ìˆ˜ íƒ€ì… ì •ë³´ í¬í•¨)
    if dominance_relations:
        print(f"     ğŸ¯ ë°œê²¬ëœ ì§€ë°° ê´€ê³„: {len(dominance_relations)}ê°œ")
        for dominator, dominated_idx, score in dominance_relations[:5]:  # ìƒìœ„ 5ê°œë§Œ ì¶œë ¥
            # ì‚¬ìš©ëœ ì ìˆ˜ íƒ€ì… í™•ì¸ (ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ)
            spec_dom = sorted_specs[dominator]
            spec_sub = sorted_specs[dominated_idx]
            
            if 'score_norm' in spec_dom and 'score_norm' in spec_sub:
                score_type = "Norm"
                qual_dom = spec_dom.get('score_norm', 0)
                qual_sub = spec_sub.get('score_norm', 0)
            elif abs(spec_dom.get('elo', 0) - spec_sub.get('elo', 0)) < 10:
                score_type = "Judge"
                qual_dom = spec_dom.get('score', 0)
                qual_sub = spec_sub.get('score', 0)
            else:
                score_type = "Elo"
                qual_dom = spec_dom.get('elo', 0)
                qual_sub = spec_sub.get('elo', 0)
            
            print(f"       #{dominator} â†’ #{dominated_idx} (ì§€ë°°ë„: {score:.2f}, {score_type}: {qual_dom:.1f}>{qual_sub:.1f})")
        if len(dominance_relations) > 5:
            print(f"       ... ì™¸ {len(dominance_relations)-5}ê°œ")
    
    # 6. ìµœì¢… ì •ë ¬ (ìµœì†Œ ë³´ì¡´ìœ¨ ê°•ì œ ë¡œì§ ì œê±°)
    survivors.sort(key=lambda s: float(s.get(score_key, 0)), reverse=True)
    
    retention_ratio = len(survivors) / len(specs) * 100
    print(f"  ğŸ¯ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: {len(specs)}ê°œ â†’ {len(survivors)}ê°œ ì„ íƒ (ë³´ì¡´ìœ¨: {retention_ratio:.1f}%)")
    
    return survivors


def calculate_dominance_relationship(spec_a, spec_b, emb_a, emb_b, similarity_threshold=0.70):
    """ë‘ spec ê°„ì˜ ì§€ë°° ê´€ê³„ íŒë‹¨ - ë„ë©”ì¸ ë¬´ê´€í•œ ê°„ë‹¨í•œ ë°©ì‹"""
    import numpy as np
    
    # 1. ê¸°ë³¸ ì¡°ê±´: ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ ë†’ì•„ì•¼ í•¨
    similarity = np.dot(emb_a, emb_b)  # ì´ë¯¸ ì •ê·œí™”ëœ ì„ë² ë”©ì´ë¯€ë¡œ ë‚´ì ì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    if similarity < similarity_threshold:
        return False, 0.0
    
    # 2. í’ˆì§ˆ ì°¨ì´ í™•ì¸ (Aê°€ Bë³´ë‹¤ ê°™ê±°ë‚˜ ë†’ì•„ì•¼ ì§€ë°° ê°€ëŠ¥)
    # ìš°ì„ ìˆœìœ„: score_norm > score > elo (ì •ê·œí™”ëœ ì ìˆ˜ê°€ ê°€ì¥ ê³µì •)
    elo_a = float(spec_a.get('elo', 0))
    elo_b = float(spec_b.get('elo', 0))
    
    # ì •ê·œí™”ëœ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if 'score_norm' in spec_a and 'score_norm' in spec_b:
        quality_a = float(spec_a.get('score_norm', 0))
        quality_b = float(spec_b.get('score_norm', 0))
        score_type = 'norm'
    # Elo ì°¨ì´ê°€ ì‘ìœ¼ë©´ judge ì ìˆ˜ ì‚¬ìš©
    elif abs(elo_a - elo_b) < 10:
        quality_a = float(spec_a.get('score', 0))
        quality_b = float(spec_b.get('score', 0))
        score_type = 'judge'
    # Elo ì°¨ì´ê°€ í¬ë©´ Elo ì ìˆ˜ ì‚¬ìš©
    else:
        quality_a = elo_a
        quality_b = elo_b
        score_type = 'elo'
    
    quality_gap = quality_a - quality_b
    
    # í’ˆì§ˆ ì°¨ì´ ì¡°ê±´: Aê°€ Bë³´ë‹¤ ë‚®ìœ¼ë©´ ì§€ë°° ë¶ˆê°€
    if quality_gap < 0:
        return False, 0.0
    
    # 3. ê°„ë‹¨í•œ ì§€ë°°ë„ ê³„ì‚° (í‚¤ì›Œë“œ ì˜ì¡´ì„± ì œê±°)
    text_a = spec_a.get('text', '')
    text_b = spec_b.get('text', '')
    
    # ê¸¸ì´ ê¸°ë°˜ ìƒì„¸ë„ (ë” ìì„¸í•œ specì´ ê°„ë‹¨í•œ specì„ ì§€ë°°)
    length_a = len(text_a.strip())
    length_b = len(text_b.strip())
    length_ratio = length_a / max(length_b, 1)
    length_advantage = min(length_ratio, 2.0)  # ìµœëŒ€ 2ë°°ê¹Œì§€ë§Œ ì¸ì •
    
    # í’ˆì§ˆ ìš°ìœ„ë„ ì •ê·œí™”
    if score_type == 'norm':  # ì •ê·œí™”ëœ ì ìˆ˜ (z-score ê¸°ë°˜)
        quality_advantage = min(quality_gap * 0.5 + 1.0, 2.0)  # 1.0ì„ ê¸°ì¤€ì ìœ¼ë¡œ
    elif score_type == 'judge':  # judge ì ìˆ˜ (0-100)
        quality_advantage = min(quality_gap / 20.0 + 1.0, 2.0)  # 20ì  ì°¨ì´ë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
    else:  # Elo ì ìˆ˜
        quality_advantage = min(quality_gap / 30.0 + 1.0, 2.0)  # 30ì  ì°¨ì´ë¥¼ 1.0ìœ¼ë¡œ ì •ê·œí™”
    
    # 4. ê°„ë‹¨í•œ ì§€ë°°ë„ ê³„ì‚° (3ê°€ì§€ ìš”ì†Œë§Œ)
    dominance_factors = {
        'similarity': similarity,           # ì˜ë¯¸ì  ìœ ì‚¬ë„ (0.7~1.0)
        'length_advantage': length_advantage,  # ê¸¸ì´ ìš°ìœ„ (1.0~2.0) 
        'quality_advantage': quality_advantage # í’ˆì§ˆ ìš°ìœ„ (1.0~2.0)
    }
    
    # ê°€ì¤‘ì¹˜ (ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¤‘ì‹¬, í’ˆì§ˆê³¼ ê¸¸ì´ëŠ” ë³´ì¡°)
    weights = {
        'similarity': 0.6,          # ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ í•µì‹¬ (60%)
        'length_advantage': 0.2,    # ê¸¸ì´ ìš°ìœ„ (20%)
        'quality_advantage': 0.2    # í’ˆì§ˆ ìš°ìœ„ (20%)
    }
    
    dominance_score = sum(
        dominance_factors[key] * weights[key] 
        for key in weights
    )
    
    # 5. ì§€ë°° íŒë‹¨ (ì„ê³„ê°’: 0.85ë¡œ ì¡°ì •)
    # similarity 0.7 * 0.6 + length_adv 1.0 * 0.2 + quality_adv 1.0 * 0.2 = 0.82 (ìµœì†Œê°’)
    # similarity 1.0 * 0.6 + length_adv 2.0 * 0.2 + quality_adv 2.0 * 0.2 = 1.4 (ìµœëŒ€ê°’)
    is_dominant = dominance_score > 0.85
    
    return is_dominant, dominance_score


# í‚¤ì›Œë“œ ê¸°ë°˜ í•¨ìˆ˜ë“¤ ì œê±°ë¨ - ë„ë©”ì¸ ë¬´ê´€í•œ ì§€ë°°ë„ ì¸¡ì •ì„ ìœ„í•´

def dedupe_by_embeddings_greedy_fallback(specs, score_key='elo', sim_threshold=0.92):
    """ê¸°ì¡´ greedy ë°©ì‹ (í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨ ì‹œ fallbackìš©)"""
    if not specs:
        return []
    texts = [mask_pii(s.get('text','')) for s in specs]
    X = embed_texts(texts)
    import numpy as _np
    order = _np.argsort([-float(s.get(score_key, 0.0)) for s in specs])
    kept, used = [], _np.zeros(len(specs), dtype=bool)
    for idx in order:
        if used[idx]: continue
        kept.append(idx); used[idx]=True
        sims = X @ X[idx]
        used |= (sims >= sim_threshold)
    return [specs[i] for i in kept]




# =========================
# (ê°œì„ ) Random Pairwise Elo with TOP-M, ìºì‹±, Elo-ê·¼ì ‘, ì ì‘í˜• ë§¤ì¹˜
# =========================

def pick_opponent_near_elo(pool: List[Dict[str, Any]], target: Dict[str, Any], k: int = 8) -> Optional[Dict[str, Any]]:
    others = [p for p in pool if p.get('id') != target.get('id')]
    if not others:
        return None
    sorted_by_gap = sorted(others, key=lambda s: abs(s.get('elo', INITIAL_ELO) - target.get('elo', INITIAL_ELO)))
    topk = sorted_by_gap[:min(k, len(sorted_by_gap))]
    return random.choice(topk) if topk else None


def matches_for_candidate(cand: Dict[str, Any], base: int = 2, floor: int = 1, ceil: int = 4) -> int:
    g = int(cand.get('games', 0))
    # ê²Œì„ì´ ë§ì„ìˆ˜ë¡ ì ì  ì¤„ì´ê³ , ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ í´ë¨í”„
    # ì˜ˆ: 0~9ê²Œì„: base, 10~19: base-1, 20~: base-2 ...
    adj = base - (g // 10)
    return max(floor, min(ceil, adj))


# =========================
# Dueling Bandit Pairwise Elo (UCB scheduling)
# =========================
def _ucb(score, games, total, c=300.0):
    import math
    return float(score) + c * math.sqrt(max(0.0, math.log(1.0 + total)) / (1.0 + games))

def run_pairwise_elo_dueling_bandit(evaluated_candidates: List[Dict[str, Any]], archive: Archive, client_judge,
                                    constitution: str, domain_profile: str, task_profile: str,
                                    cfg: EvolverConfig, generation: int, judges_dir: str):
    if not evaluated_candidates:
        return
    log_path = os.path.join(judges_dir, f"gen{generation:03d}_pairwise_elo.jsonl")

    for s in evaluated_candidates:
        s.setdefault('elo', cfg.elo_initial)
        s.setdefault('games', 0); s.setdefault('wins', 0); s.setdefault('losses', 0); s.setdefault('draws', 0)

    pool = sorted(evaluated_candidates, key=lambda s: s.get('elo', cfg.elo_initial), reverse=True)[:max(1, cfg.pairwise_top_m)]
    total_duels = 0
    min_games = max(1, getattr(cfg, 'db_min_games', 2))
    max_duels = max(50, getattr(cfg, 'db_max_duels', 3000))
    stop_gap = max(10, getattr(cfg, 'db_stop_gap', 50))
    c_ucb = float(getattr(cfg, 'db_ucb_c', 300.0))

    while total_duels < max_duels:
        under = [s for s in pool if int(s.get('games',0)) < min_games]
        if len(under) >= 2:
            A = max(under, key=lambda s: s.get('elo', cfg.elo_initial))
            others = [x for x in under if x['id'] != A['id']]
            if not others:
                others = [x for x in pool if x['id'] != A['id']]
            B = min(others, key=lambda s: abs(s.get('elo', cfg.elo_initial) - A.get('elo', cfg.elo_initial)))
        else:
            total = sum(int(s.get('games',0)) for s in pool) + 1
            scores = [(s, _ucb(s.get('elo', cfg.elo_initial), int(s.get('games',0)), total, c_ucb)) for s in pool]
            scores.sort(key=lambda t: t[1], reverse=True)
            A = scores[0][0]
            candidates = [t[0] for t in scores[1:8]] or [t[0] for t in scores[1:]]
            B = min(candidates, key=lambda s: abs(s.get('elo', cfg.elo_initial) - A.get('elo', cfg.elo_initial)))

        outcome_a = pairwise_ab_ba_conditional(client_judge, A['text'], B['text'],
                                               constitution, domain_profile, task_profile,
                                               use_ab_ba=cfg.use_ab_ba)
        r_a_old, r_b_old = A['elo'], B['elo']
        k_val = cfg.elo_k
        if cfg.elo_dynamic_k:
            k_val = effective_k(cfg.elo_k, r_a_old, r_b_old, A['games'], B['games'], cfg.elo_min_k)
        r_a_new, r_b_new = update_elo(r_a_old, r_b_old, outcome_a, k=k_val)
        A['elo'] = r_a_new; B['elo'] = r_b_new
        A['games'] += 1; B['games'] += 1
        if outcome_a == 1.0:
            A['wins'] += 1; B['losses'] += 1
        elif outcome_a == 0.0:
            A['losses'] += 1; B['wins'] += 1
        else:
            A['draws'] += 1; B['draws'] += 1

        total_duels += 1

        try:
            with open(log_path, 'a', encoding='utf-8') as lf:
                lf.write(json.dumps({
                    'generation': generation,
                    'timestamp': time.time(),
                    'mode': 'DuelingBandit-UCB',
                    'duel': total_duels,
                    'A': A.get('id'), 'B': B.get('id'),
                    'A_elo_before': r_a_old, 'B_elo_before': r_b_old,
                    'A_elo_after': A['elo'], 'B_elo_after': B['elo'],
                    'outcome_a': outcome_a, 'k': k_val,
                }, ensure_ascii=False) + "\n")
        except Exception as e:
            print(f"(warn) dueling bandit log write failed: {e}")

        pool.sort(key=lambda s: s.get('elo', cfg.elo_initial), reverse=True)
        if len(pool) >= 2 and (pool[0]['elo'] - pool[1]['elo']) >= stop_gap and total_duels >= len(pool):
            break

    archive.specs.sort(key=archive._rank_key, reverse=True)



# Evolution loop
# =========================

def run_task_evolution(task_name: str, initial_specs: List[Dict[str, Any]], constitution: str, domain_profile: str, task_profile: str, cfg: EvolverConfig, base_output_dir: str = None, domain_name: str = None, domain_concepts: List[str] = None, task_concepts: List[str] = None):
    # ëª¨ë¸ ì„ íƒ
    if cfg.generator_model.startswith('gpt'):
        client_gen = OpenAIClientWrapper(api_key=cfg.openai_api_key, model=cfg.generator_model)
    else:
        client_gen = AnthropicClientWrapper(api_key=cfg.anthropic_api_key, model=cfg.generator_model)

    if cfg.judge_model.startswith('gpt'):
        client_judge = OpenAIClientWrapper(api_key=cfg.openai_api_key, model=cfg.judge_model)
    else:
        client_judge = AnthropicClientWrapper(api_key=cfg.anthropic_api_key, model=cfg.judge_model)

    judges = {
        'constitution': Judge('constitution', client_judge),
        'domain': Judge('domain', client_judge),
        'task': Judge('task', client_judge),
    }

    archive = Archive(max_capacity=100, elo_initial=cfg.elo_initial)
    random.seed(cfg.random_seed)

    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ (base_output_dirì´ ì „ë‹¬ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
    if base_output_dir is None:
        base_output_dir = cfg.output_dir
    
    # ë””ë ‰í„°ë¦¬ êµ¬ì¡°: base_output_dir/domain/task/
    if domain_name:
        # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ë¥¼ ë¶„ë¦¬í•´ì„œ ì €ì¥
        safe_domain = domain_name.replace(' & ', '_and_').replace(' ', '_')
        safe_task = task_name.replace(' & ', '_and_').replace(' ', '_')
        out_dir = os.path.join(base_output_dir, safe_domain, safe_task)
    else:
        # ê¸°ì¡´ ë°©ì‹ (í˜¸í™˜ì„±)
        out_dir = os.path.join(base_output_dir, task_name.replace(' ', '_'))
    ensure_dir(out_dir)
    best_dir = os.path.join(out_dir, 'best')
    generator_dir = os.path.join(out_dir, 'generator')
    judges_dir = os.path.join(out_dir, 'judges')
    ensure_dir(best_dir); ensure_dir(generator_dir); ensure_dir(judges_dir)

    population = []
    for s in initial_specs:
        seed_text = s.get('text', '')
        individual_seed_specs = split_llm_response_to_specs(seed_text, 'seed')
        print(f"ğŸ’¡ ì´ˆê¸° seedë¥¼ {len(individual_seed_specs)}ê°œ ê°œë³„ specìœ¼ë¡œ ë¶„ë¦¬")
        print(f"  âš–ï¸ ê°œë³„ spec í‰ê°€ ì¤‘... (ì´ {len(individual_seed_specs)}ê°œ)")
        for i, individual_spec in enumerate(individual_seed_specs):
            individual_spec['id'] = make_unique_id(individual_spec['text'], generation=-1, index=i)
            individual_spec['meta'] = {'origin': 'seed_split', 'index': i}
            print(f"    í‰ê°€ ì¤‘: {i+1}/{len(individual_seed_specs)} - \"{individual_spec['text'][:50]}...\"")
            is_top10 = i < 10
            evaluated = evaluate_spec_with_judges(
                individual_spec, judges, cfg.judge_weights,
                constitution, domain_profile, task_profile,
                -1, 3, judges_dir if is_top10 else None, is_top10,
                initial_elo=cfg.elo_initial
            )
            archive.add(evaluated)
            population.append(evaluated)
        print(f"  âœ… ì´ˆê¸° í‰ê°€ ì™„ë£Œ! Archiveì— {len(population)}ê°œ spec ì €ì¥ë¨")
        if archive.specs:
            top_elo = archive.specs[0].get('elo', cfg.elo_initial)
            bottom_elo = archive.specs[-1].get('elo', cfg.elo_initial)
            print(f"  ğŸ“Š Archive ì €ì¥: {len(archive.specs)}ê°œ (ìµœê³  Elo:{top_elo:.1f}, ìµœì € Elo:{bottom_elo:.1f})")

    history_path = os.path.join(out_dir, 'history.jsonl')

    with open(history_path, 'a', encoding='utf-8') as hf:
        for gen in range(cfg.generations):
            print(f"\nğŸš€ [{task_name}] Generation {gen + 1}/{cfg.generations}")
            print("=" * 60)

            # ë¶€ëª¨ ìƒ˜í”Œë§ í›„ ë³€ì´ ìƒì„±
            all_archive_specs = archive.all_elites()
            print(f"  ğŸ§¬ ë³€ì´ ìƒì„± ì¤‘... (Archive {len(all_archive_specs)}ê°œ, parents {cfg.parent_selection_size}ê°œì”© â†’ {cfg.population_per_gen}ë²ˆ)")
            all_candidate_specs: List[Dict[str, Any]] = []
            for i in range(cfg.population_per_gen):
                if cfg.use_task_diversity:
                    selected_parents = archive.sample_parents_task_diverse(
                        n=cfg.parent_selection_size,
                        mix=cfg.diverse_parent_mix,
                        task_pool_size=cfg.task_pool_size,
                        metric=(cfg.parent_selection_metric or "task")
                    )
                else:
                    top_30_specs = all_archive_specs[:30]
                    selected_parents = random.sample(top_30_specs, min(cfg.parent_selection_size, len(top_30_specs))) if top_30_specs else []
                print(f"    ë³€ì´ {i+1}/{cfg.population_per_gen}: ë¶€ëª¨ {len(selected_parents)}ê°œ ì¡°í•© ì¤‘...")
                child_specs = apply_variation_multi_parent(selected_parents, client_gen, constitution, domain_profile, task_profile, gen, generator_dir, domain_name, task_name, domain_concepts, task_concepts)
                # ì´ˆê¸° Elo ì„¸íŒ…
                for cs in child_specs:
                    cs.setdefault('elo', cfg.elo_initial)
                    cs.setdefault('games', 0); cs.setdefault('wins', 0); cs.setdefault('losses', 0); cs.setdefault('draws', 0)
                all_candidate_specs.extend(child_specs)
                print(f"    â†’ {len(child_specs)}ê°œ spec ìƒì„±ë¨")
            print(f"  ğŸ“Š ì´ {len(all_candidate_specs)}ê°œ í›„ë³´ spec ìƒì„± ì™„ë£Œ")
            
            # í¬ì¸íŠ¸ì™€ì´ì¦ˆ í‰ê°€(ì°¸ê³ Â·ë¡œê¹…ìš©) - ë¨¼ì € í‰ê°€!
            print(f"  âš–ï¸ í‰ê°€ ì¤‘... ({len(all_candidate_specs)}ê°œ spec â†’ {cfg.parallel_workers}ê°œ ë³‘ë ¬)")
            evaluated_candidates: List[Dict[str, Any]] = []
            completed_count = 0
            with ThreadPoolExecutor(max_workers=cfg.parallel_workers) as ex:
                futures = {}
                for i, candidate in enumerate(all_candidate_specs):
                    is_top10 = i < 10
                    future = ex.submit(
                        evaluate_spec_with_judges, candidate, judges, cfg.judge_weights,
                        constitution, domain_profile, task_profile, gen, 3, judges_dir, is_top10,
                        cfg.elo_initial
                    )
                    futures[future] = candidate
                for fut in as_completed(futures):
                    try:
                        res = fut.result()
                        evaluated_candidates.append(res)
                        completed_count += 1
                        if completed_count % max(1, len(all_candidate_specs) // 4) == 0 or completed_count == len(all_candidate_specs):
                            print(f"    í‰ê°€ ì§„í–‰: {completed_count}/{len(all_candidate_specs)} ì™„ë£Œ ({completed_count/len(all_candidate_specs)*100:.1f}%)")
                    except Exception as e:
                        print(f'    âš ï¸ í‰ê°€ ì˜¤ë¥˜: {e}')
            print(f"  âœ… í‰ê°€ ì™„ë£Œ! {len(evaluated_candidates)}ê°œ spec í‰ê°€ë¨")
            
            # === Judge ì ìˆ˜ë¡œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì§€ë°° ê´€ê³„ ì¤‘ë³µ ì œê±° ===
            before = len(evaluated_candidates)
            evaluated_candidates = priority_hierarchical_dedup(
                evaluated_candidates, 
                score_key='score',  # Judge ì ìˆ˜ ì‚¬ìš©!
                keep_ratio=0.4,  # 40% ë³´ì¡´
                similarity_threshold=0.70  # ê°œì„ ëœ ì„ë² ë”© ëª¨ë¸ì— ë§ëŠ” ì—„ê²©í•œ ê¸°ì¤€
            )

            # PATCH: ì‹¬íŒ ì ìˆ˜ ì •ê·œí™” (ì•„ì¹´ì´ë¸Œ+í›„ë³´ í’€ì— ì ìš©) â†’ í¸í–¥ ì™„í™”
            if cfg.use_score_normalization:
                pool_for_norm = archive.all_elites() + evaluated_candidates
                normalize_judge_scores_for_pool(pool_for_norm)

            # === Pairwise Elo ë¼ìš´ë“œ ===
            if cfg.use_pairwise_elo:
                print(f"  ğŸ¥Š Pairwise-Elo ì‹œì‘... (mode=DuelingBandit, dynK={cfg.elo_dynamic_k})")
                run_pairwise_elo_dueling_bandit(
                    evaluated_candidates, archive, client_judge,
                    constitution, domain_profile, task_profile,
                    cfg, generation=gen, judges_dir=judges_dir
                )
                print(f"  âœ… Pairwise-Elo ì™„ë£Œ")

            # ì•„ì¹´ì´ë¸Œì— í›„ë³´ ì¶”ê°€ (ì •ë ¬ì€ Elo ìš°ì„ , íƒ€ì´ë¸Œë ˆì´í¬=score_norm)
            print(f"  ğŸ¯ ì„ ë³„/ì¶”ê°€ ì¤‘... (Elo ìš°ì„  ì •ë ¬)")
            for candidate in evaluated_candidates:
                archive.add(candidate)

            elites = archive.all_elites()
            best_elo = elites[0].get('elo', cfg.elo_initial) if elites else cfg.elo_initial
            
            # Elo dispersion metrics (no averages)
            elos = [e.get('elo', cfg.elo_initial) for e in evaluated_candidates]
            import statistics as st
            elo_std = st.pstdev(elos) if len(elos) > 1 else 0.0
            elo_min = min(elos, default=cfg.elo_initial)
            elo_max = max(elos, default=cfg.elo_initial)
            elo_p90 = float(np.percentile(elos, 90)) if elos else cfg.elo_initial
            elo_p10 = float(np.percentile(elos, 10)) if elos else cfg.elo_initial
            # elos, elo_std, elo_min/max, elo_p90/p10 ê³„ì‚° ë°”ë¡œ ì•„ë˜ì— ì¶”ê°€
            avg_elo = float(np.mean(elos)) if elos else cfg.elo_initial

            record = {
                'generation': gen,
                'best_elo': best_elo,
                'elo_std': elo_std,
                'elo_min': elo_min,
                'elo_max': elo_max,
                'elo_range': elo_max - elo_min,
                'elo_p90': elo_p90,
                'elo_p10': elo_p10,
                'archive_size': len(archive.specs),
                'timestamp': time.time(),
            }
            hf.write(json.dumps(record, ensure_ascii=False) + "\n")
            hf.flush()


            # ìƒìœ„ 30ê°œ ì €ì¥ (Elo í¬í•¨, ìœ ì˜ì„± ë§ˆí¬)
            fname = os.path.join(best_dir, f"gen{gen:03d}_top30.md")
            with open(fname, 'w', encoding='utf-8') as wf:
                wf.write(f"# Generation {gen} - Top 30 Specs\n\n")
                wf.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                wf.write(f"Archive size: {len(archive.specs)}\n")
                wf.write(f"Best Elo: {best_elo:.1f}\n")
                wf.write(f"Elo p90 / p10: {elo_p90:.1f} / {elo_p10:.1f}\n")
                wf.write(f"Elo std: {elo_std:.1f} | range: {elo_min:.1f}â€“{elo_max:.1f} (Î”{(elo_max-elo_min):.1f})\n\n")
                wf.write("=" * 80 + "\n\n")
                prev_elo = None
                for i, s in enumerate(elites[:30]):
                    elo = s.get('elo', cfg.elo_initial)
                    gap_flag = ""
                    if prev_elo is not None and (prev_elo - elo) < cfg.significance_gap:
                        gap_flag = " (â‰ˆtie)"  # ìœ ì˜ì„± ë¶€ì¡±
                    prev_elo = elo
                    wf.write(f"## Rank #{i+1}{gap_flag}\n\n")
                    wf.write(f"**ID:** {s['id']}\n")
                    wf.write(f"**Elo:** {elo:.1f} (W-D-L: {s.get('wins',0)}-{s.get('draws',0)}-{s.get('losses',0)}, Games: {s.get('games',0)})\n")
                    wf.write(f"**Score(Ref):** {s.get('score',0)}/100  |  **Score(norm):** {s.get('score_norm', 0.0):+.2f}\n")
                    wf.write(f"**Breakdown:** Constitution: {s.get('scores',{}).get('constitution',0)}/40, Domain: {s.get('scores',{}).get('domain',0)}/30, Task: {s.get('scores',{}).get('task',0)}/30\n")
                    provenance = s.get('provenance', [])
                    if provenance:
                        wf.write(f"**Evolution:** {' â†’ '.join([p['op'] for p in provenance])}\n")
                    else:
                        wf.write(f"**Evolution:** [Original Seed]\n")
                    wf.write(f"\n**Spec Text:**\n")
                    wf.write(f"```\n{strip_leading_numbering(s['text'])}\n```\n\n")
                    wf.write("-" * 60 + "\n\n")

            if domain_name:
                task_display = f"{domain_name}/{task_name}"
            else:
                task_display = task_name
            print(f" [{task_display}] Gen {gen}: archive_best_elo={best_elo:.1f} avg_elo={avg_elo:.1f} archive_size={len(archive.specs)} candidates={len(evaluated_candidates)} added={len(evaluated_candidates)} mode=DuelingBandit")

    if domain_name:
        task_display = f"{domain_name}/{task_name}"
    else:
        task_display = task_name
    print(f"[{task_display}] Evolution finished. Results in {out_dir}")
    return archive


# =========================
# IO helpers
# =========================

def load_text_file(path: str) -> str:
    if not os.path.exists(path):
        print(f"Warning: {path} not found. Returning empty string.")
        return ""
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()


def load_tasks_json(path: str) -> Dict[str, Dict[str, Any]]:
    if not os.path.exists(path):
        print(f"Warning: {path} not found. Returning empty dict.")
        return {}
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    # í˜¸í™˜ì„±: ê¸°ì¡´ ë¬¸ìì—´ í˜•ì‹ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    for key, value in data.items():
        if isinstance(value, str):
            data[key] = {"description": value, "core_concepts": []}
    return data


def load_domains_json(path: str) -> Dict[str, Dict[str, Any]]:
    if not os.path.exists(path):
        print(f"Warning: {path} not found. Returning empty dict.")
        return {}
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    # í˜¸í™˜ì„±: ê¸°ì¡´ ë¬¸ìì—´ í˜•ì‹ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    for key, value in data.items():
        if isinstance(value, str):
            data[key] = {"description": value, "core_concepts": []}
    return data


# =========================
# Entrypoint (auto-load one task)
# =========================

def run_domain_tasks_auto_load(cfg: EvolverConfig, target_domain: str = "Legal_and_Regulatory", few_shot_folder: str = "few_shot_examples"):
    """íŠ¹ì • ë„ë©”ì¸ì˜ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ SPEC ì§„í™”ë¥¼ ì‹¤í–‰"""
    constitution = load_text_file("constitution.txt") or "Basic constitution for safe AI."
    domains = load_domains_json("domains.json")
    if not domains:
        domains = {"General": {"description": "General purpose domain for testing", "core_concepts": []}}
    tasks = load_tasks_json("tasks.json")
    if not tasks:
        tasks = {"Test Task": {"description": "A minimal test task for SPEC evolution", "core_concepts": []}}

    ensure_dir(few_shot_folder)

    # ë„ë©”ì¸ ë°ì´í„° í™•ì¸
    domain_name = target_domain.replace('_and_', ' & ').replace('_', ' ')
    if domain_name not in domains:
        print(f"âŒ ë„ë©”ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {domain_name}")
        print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë„ë©”ì¸ë“¤: {list(domains.keys())}")
        return

    domain_data = domains[domain_name]
    domain_desc = domain_data.get("description", str(domain_data))
    domain_concepts = domain_data.get("core_concepts", [])

    print(f"ğŸš€ ë„ë©”ì¸ '{domain_name}'ì˜ ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰ ì‹œì‘")
    print(f"ğŸ“Š ë„ë©”ì¸ ì„¤ëª…: {domain_desc}")

    # í•´ë‹¹ ë„ë©”ì¸ì˜ few_shot í´ë”ì—ì„œ ëª¨ë“  íƒœìŠ¤í¬ ì°¾ê¸°
    safe_domain = target_domain
    domain_folder = os.path.join(few_shot_folder, safe_domain)
    ensure_dir(domain_folder)

    # í•´ë‹¹ ë„ë©”ì¸ í´ë”ì˜ ëª¨ë“  .txt íŒŒì¼ ì°¾ê¸° (ê°ê°ì´ íƒœìŠ¤í¬)
    if not os.path.exists(domain_folder):
        print(f"âŒ ë„ë©”ì¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {domain_folder}")
        return

    task_files = [f for f in os.listdir(domain_folder) if f.endswith('.txt')]
    if not task_files:
        print(f"âš ï¸ {domain_folder}ì— íƒœìŠ¤í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“‹ ë°œê²¬ëœ íƒœìŠ¤í¬ë“¤: {task_files}")

    # ê° íƒœìŠ¤í¬ì— ëŒ€í•´ ì§„í™” ì‹¤í–‰
    for task_file in task_files:
        task_name = task_file.replace('.txt', '').replace('_and_', ' & ').replace('_', ' ')
        print(f"\nğŸ¯ íƒœìŠ¤í¬ ì‹¤í–‰: {task_name}")

        # íƒœìŠ¤í¬ ë°ì´í„° í™•ì¸
        task_data = tasks.get(task_name, {})
        task_desc = task_data.get("description", f"Task: {task_name}")
        task_concepts = task_data.get("core_concepts", [])

        safe_task = task_file.replace('.txt', '')
        combination_name = f"{safe_domain}__{safe_task}"

        file_name = os.path.join(domain_folder, task_file)

        task_profile = f"### Task: {task_name}\n- Description: {task_desc}\n"
        domain_profile = f"### Domain: {domain_name}\n- Description: {domain_desc}\n"

        initial_specs: List[Dict[str, Any]] = []
        if os.path.exists(file_name):
            with open(file_name, 'r', encoding='utf-8') as f:
                txt = f.read()
                initial_specs.append({'text': txt})
            print(f"  âœ… Found existing spec file: {file_name}")
        else:
            minimal = f"## SPEC DRAFT for {combination_name}\n\nDomain: {domain_name}\nTask: {task_name}\n\nRULES:\n- MUST: follow instructions.\n"
            initial_specs.append({'text': minimal})
            print(f"  âš ï¸ No spec file found, using minimal spec: {file_name}")

        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„±
        if cfg.use_timestamp_suffix:
            timestamp_suffix = time.strftime("_%Y%m%d_%H%M%S")
            base_output_dir = cfg.output_dir + timestamp_suffix
        else:
            base_output_dir = cfg.output_dir

        print(f"  ğŸ“ Output directory: {base_output_dir}")
        archive = run_task_evolution(task_name, initial_specs, constitution, domain_profile, task_profile, cfg, base_output_dir, domain_name, domain_concepts, task_concepts)

        elites = archive.all_elites()
        evolved_elites = [spec for spec in elites if spec.get('provenance', [])]

        if evolved_elites:
            best_evolved_spec = evolved_elites[0]
            print(f"    ğŸ‰ Best Evolved Spec (Seed ì œì™¸)")
            print(f"       Elo: {best_evolved_spec.get('elo', cfg.elo_initial):.1f}")
            print(f"       Score: {best_evolved_spec.get('score',0)}/100")
        else:
            print("    âš ï¸ No evolved specs found.")

        if elites:
            print(f"    ğŸ“Š Total specs: {len(elites)}, Evolved specs: {len(evolved_elites)}")
            print(f"    ğŸ† Highest Elo: {elites[0].get('elo', cfg.elo_initial):.1f}")

            # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê³„ì‚° (run_task_evolutionê³¼ ë™ì¼í•œ ë¡œì§)
            safe_domain = domain_name.replace(' & ', '_and_').replace(' ', '_')
            safe_task = task_name.replace(' & ', '_and_').replace(' ', '_')
            task_out_dir = os.path.join(base_output_dir, safe_domain, safe_task)
            ensure_dir(task_out_dir)
            archive_file_json = os.path.join(task_out_dir, 'top100_archive.json')
            archive_file_txt = os.path.join(task_out_dir, 'top100_archive.txt')

            # === ìµœì¢… ì¶œë ¥ ì „ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ===
            elites = priority_hierarchical_dedup(
                elites,
                score_key='elo',
                keep_ratio=0.6,  # ìµœì¢… ì•„ì¹´ì´ë¸ŒëŠ” 60% ë³´ì¡´
                similarity_threshold=0.65  # ìµœì¢…ì—ì„œëŠ” ì•½ê°„ ê´€ëŒ€í•œ ê¸°ì¤€ (ì„¸ëŒ€ë³„ë³´ë‹¤ ì™„í™”)
            )
            archive_data = []
            for i, spec in enumerate(elites):
                archive_data.append({
                    'rank': i + 1,
                    'id': spec['id'],
                    'elo': spec.get('elo', cfg.elo_initial),
                    'games': spec.get('games', 0),
                    'wins': spec.get('wins', 0),
                    'losses': spec.get('losses', 0),
                    'draws': spec.get('draws', 0),
                    'score': spec.get('score', 0),
                    'score_norm': spec.get('score_norm', 0.0),
                    'scores': spec.get('scores', {}),
                    'provenance': spec.get('provenance', []),
                    'text': strip_leading_numbering(spec['text']),
                    'evaluated_at': spec.get('evaluated_at', ''),
                    'is_evolved': len(spec.get('provenance', [])) > 0
                })
            with open(archive_file_json, 'w', encoding='utf-8') as f:
                json.dump(archive_data, f, ensure_ascii=False, indent=2)

            with open(archive_file_txt, 'w', encoding='utf-8') as f:
                f.write(f"=== Top {len(elites)} Archive Specs ===\n")
                f.write(f"Domain: {domain_name}\n")
                f.write(f"Task: {task_name}\n")
                f.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Total specs: {len(elites)}, Evolved specs: {len(evolved_elites)}\n\n")
                prev_elo = None
                for i, spec in enumerate(elites):
                    elo = spec.get('elo', cfg.elo_initial)
                    gap_flag = ""
                    if prev_elo is not None and (prev_elo - elo) < cfg.significance_gap:
                        gap_flag = " (â‰ˆtie)"
                    prev_elo = elo
                    f.write(f"{'='*60}\n")
                    f.write(f"RANK #{i+1}{gap_flag}\n")
                    f.write(f"ID: {spec['id']}\n")
                    f.write(f"ELO: {elo:.1f} (W-D-L: {spec.get('wins',0)}-{spec.get('draws',0)}-{spec.get('losses',0)}, Games: {spec.get('games',0)})\n")
                    f.write(f"SCORE(REF): {spec.get('score',0)}/100  |  SCORE(NORM): {spec.get('score_norm', 0.0):+.2f}\n")
                    f.write(f"  - Constitution: {spec.get('scores',{}).get('constitution',0)}/40\n")
                    f.write(f"  - Domain: {spec.get('scores',{}).get('domain',0)}/30\n")
                    f.write(f"  - Task: {spec.get('scores',{}).get('task',0)}/30\n")
                    provenance = spec.get('provenance', [])
                    if provenance:
                        f.write(f"EVOLUTION: {' â†’ '.join([p['op'] for p in provenance])}\n")
                    else:
                        f.write(f"EVOLUTION: [Original Seed]\n")
                    f.write(f"\nSPEC TEXT:\n")
                    f.write(f"{strip_leading_numbering(spec['text'])}\n\n")
        print(f"ğŸ“ Archive ì €ì¥ ì™„ë£Œ:")
        print(f"  - JSON: {archive_file_json}")
        print(f"  - TXT: {archive_file_txt}")


def run_single_task_evolution(target_domain: str, target_task: str):
    """íŠ¹ì • ë„ë©”ì¸ì˜ íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•´ì„œë§Œ ë‹¨ì¼ ìŠ¤í™ ì§„í™”ë¥¼ ì‹¤í–‰"""
    import os
    
    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "")
    openai_api_key = os.environ.get("OPENAI_API_KEY", "")
    config = EvolverConfig(
        anthropic_api_key=anthropic_api_key,
        openai_api_key=openai_api_key,
        use_timestamp_suffix=False
    )

    constitution = load_text_file("constitution.txt") or "Basic constitution for safe AI."
    domains = load_domains_json("domains.json")
    if not domains:
        domains = {"General": {"description": "General purpose domain for testing", "core_concepts": []}}
    tasks = load_tasks_json("tasks.json")
    if not tasks:
        tasks = {"Test Task": {"description": "A minimal test task for SPEC evolution", "core_concepts": []}}

    few_shot_folder = "few_shot_examples"
    ensure_dir(few_shot_folder)

    # ë„ë©”ì¸ ë°ì´í„° í™•ì¸
    domain_name = target_domain.replace('_and_', ' & ').replace('_', ' ')
    if domain_name not in domains:
        print(f"âŒ ë„ë©”ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {domain_name}")
        print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë„ë©”ì¸ë“¤: {list(domains.keys())}")
        return

    domain_data = domains[domain_name]
    domain_desc = domain_data.get("description", str(domain_data))
    domain_concepts = domain_data.get("core_concepts", [])

    # íƒœìŠ¤í¬ ë°ì´í„° í™•ì¸
    task_name = target_task.replace('_and_', ' & ').replace('_', ' ')
    task_data = tasks.get(task_name, {})
    task_desc = task_data.get("description", f"Task: {task_name}")
    task_concepts = task_data.get("core_concepts", [])

    print(f"ğŸš€ ë‹¨ì¼ íƒœìŠ¤í¬ ìŠ¤í™ ì§„í™”: {domain_name} / {task_name}")

    safe_domain = target_domain
    safe_task = target_task
    combination_name = f"{safe_domain}__{safe_task}"

    domain_folder = os.path.join(few_shot_folder, safe_domain)
    ensure_dir(domain_folder)
    file_name = os.path.join(domain_folder, f"{safe_task}.txt")

    task_profile = f"### Task: {task_name}\n- Description: {task_desc}\n"
    domain_profile = f"### Domain: {domain_name}\n- Description: {domain_desc}\n"

    initial_specs = []
    if os.path.exists(file_name):
        with open(file_name, 'r', encoding='utf-8') as f:
            txt = f.read()
            initial_specs.append({'text': txt})
        print(f"  âœ… Found existing spec file: {file_name}")
    else:
        minimal = f"## SPEC DRAFT for {combination_name}\n\nDomain: {domain_name}\nTask: {task_name}\n\nRULES:\n- MUST: follow instructions.\n"
        initial_specs.append({'text': minimal})
        print(f"  âš ï¸ No spec file found, using minimal spec: {file_name}")

    base_output_dir = config.output_dir
    print(f"  ğŸ“ Output directory: {base_output_dir}")
    
    archive = run_task_evolution(task_name, initial_specs, constitution, domain_profile, task_profile, config, base_output_dir, domain_name, domain_concepts, task_concepts)

    elites = archive.all_elites()
    evolved_elites = [spec for spec in elites if spec.get('provenance', [])]

    if evolved_elites:
        best_evolved_spec = evolved_elites[0]
        print(f"    ğŸ‰ Best Evolved Spec (Seed ì œì™¸)")
        print(f"       Elo: {best_evolved_spec.get('elo', config.elo_initial):.1f}")
        print(f"       Score: {best_evolved_spec.get('score',0)}/100")
    else:
        print("    âš ï¸ No evolved specs found.")

    if elites:
        print(f"    ğŸ“Š Total specs: {len(elites)}, Evolved specs: {len(evolved_elites)}")
        print(f"    ğŸ† Highest Elo: {elites[0].get('elo', config.elo_initial):.1f}")

        # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ê³„ì‚° (run_task_evolutionê³¼ ë™ì¼í•œ ë¡œì§)
        safe_domain = domain_name.replace(' & ', '_and_').replace(' ', '_')
        safe_task = task_name.replace(' & ', '_and_').replace(' ', '_')
        task_out_dir = os.path.join(base_output_dir, safe_domain, safe_task)
        ensure_dir(task_out_dir)
        archive_file_json = os.path.join(task_out_dir, 'top100_archive.json')
        archive_file_txt = os.path.join(task_out_dir, 'top100_archive.txt')

        # === ìµœì¢… ì¶œë ¥ ì „ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° ===
        elites = priority_hierarchical_dedup(
            elites,
            score_key='elo',
            keep_ratio=0.6,  # ìµœì¢… ì•„ì¹´ì´ë¸ŒëŠ” 60% ë³´ì¡´
            similarity_threshold=0.65  # ìµœì¢…ì—ì„œëŠ” ì•½ê°„ ê´€ëŒ€í•œ ê¸°ì¤€ (ì„¸ëŒ€ë³„ë³´ë‹¤ ì™„í™”)
        )
        archive_data = []
        for i, spec in enumerate(elites):
            archive_data.append({
                'rank': i + 1,
                'id': spec['id'],
                'elo': spec.get('elo', config.elo_initial),
                'games': spec.get('games', 0),
                'wins': spec.get('wins', 0),
                'losses': spec.get('losses', 0),
                'draws': spec.get('draws', 0),
                'score': spec.get('score', 0),
                'score_norm': spec.get('score_norm', 0.0),
                'scores': spec.get('scores', {}),
                'provenance': spec.get('provenance', []),
                'text': strip_leading_numbering(spec['text']),
                'evaluated_at': spec.get('evaluated_at', ''),
                'is_evolved': len(spec.get('provenance', [])) > 0
            })
        with open(archive_file_json, 'w', encoding='utf-8') as f:
            json.dump(archive_data, f, ensure_ascii=False, indent=2)

        with open(archive_file_txt, 'w', encoding='utf-8') as f:
            f.write(f"=== Top {len(elites)} Archive Specs ===\n")
            f.write(f"Domain: {domain_name}\n")
            f.write(f"Task: {task_name}\n")
            f.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Total specs: {len(elites)}, Evolved specs: {len(evolved_elites)}\n\n")
            prev_elo = None
            for i, spec in enumerate(elites):
                elo = spec.get('elo', config.elo_initial)
                gap_flag = ""
                if prev_elo is not None and (prev_elo - elo) < config.significance_gap:
                    gap_flag = " (â‰ˆtie)"
                prev_elo = elo
                f.write(f"{'='*60}\n")
                f.write(f"RANK #{i+1}{gap_flag}\n")
                f.write(f"ID: {spec['id']}\n")
                f.write(f"ELO: {elo:.1f} (W-D-L: {spec.get('wins',0)}-{spec.get('draws',0)}-{spec.get('losses',0)}, Games: {spec.get('games',0)})\n")
                f.write(f"SCORE(REF): {spec.get('score',0)}/100  |  SCORE(NORM): {spec.get('score_norm', 0.0):+.2f}\n")
                f.write(f"  - Constitution: {spec.get('scores',{}).get('constitution',0)}/40\n")
                f.write(f"  - Domain: {spec.get('scores',{}).get('domain',0)}/30\n")
                f.write(f"  - Task: {spec.get('scores',{}).get('task',0)}/30\n")
                provenance = spec.get('provenance', [])
                if provenance:
                    f.write(f"EVOLUTION: {' â†’ '.join([p['op'] for p in provenance])}\n")
                else:
                    f.write(f"EVOLUTION: [Original Seed]\n")
                f.write(f"\nSPEC TEXT:\n")
                f.write(f"{strip_leading_numbering(spec['text'])}\n\n")

        print(f"ğŸ“ Archive ì €ì¥ ì™„ë£Œ:")
        print(f"  - JSON: {archive_file_json}")
        print(f"  - TXT: {archive_file_txt}")


if __name__ == '__main__':
    import sys

    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "")
    openai_api_key = os.environ.get("OPENAI_API_KEY", "")
    config = EvolverConfig(
        anthropic_api_key=anthropic_api_key,
        openai_api_key=openai_api_key,
        use_timestamp_suffix=False  # íƒ€ì„ìŠ¤íƒ¬í”„ í´ë”ëª… ì‚¬ìš© ì•ˆí•¨
    )

    if len(sys.argv) >= 3:
        # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ ë‘˜ ë‹¤ ì§€ì •ëœ ê²½ìš°
        target_domain = sys.argv[1]
        target_task = sys.argv[2]
        print(f"ğŸ¯ Target: {target_domain} / {target_task}")
        run_single_task_evolution(target_domain, target_task)
    elif len(sys.argv) >= 2:
        # ë„ë©”ì¸ë§Œ ì§€ì •ëœ ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
        target_domain = sys.argv[1]
        print(f"ğŸ¯ Target Domain: {target_domain} (ëª¨ë“  íƒœìŠ¤í¬)")
        run_domain_tasks_auto_load(config, target_domain=target_domain)
    else:
        # ì•„ë¬´ê²ƒë„ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš°
        print("ğŸ¯ Default: Legal_and_Regulatory (ëª¨ë“  íƒœìŠ¤í¬)")
        run_domain_tasks_auto_load(config, target_domain="Legal_and_Regulatory")