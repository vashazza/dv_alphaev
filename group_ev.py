"""
Group-based Specification Evolution Pipeline
ì§‘ë‹¨ spec í‰ê°€ ë° ì§„í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸

ì£¼ìš” íŠ¹ì§•:
- ê°œë³„ specì´ ì•„ë‹Œ spec ì§‘ë‹¨(ê·¸ë£¹) ë‹¨ìœ„ë¡œ í‰ê°€
- ìœ ê¸°ì  ë™ì‘ì„±, ë¦¬ìŠ¤í¬ ì»¤ë²„ë¦¬ì§€, ì¤‘ë³µì„±, ì‹¤ìš©ì„± ë“±ì„ ì¢…í•© í‰ê°€
- ê°•ë ¥í•œ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì˜ë¯¸ì ìœ¼ë¡œ coherentí•œ ê·¸ë£¹ í˜•ì„±
- ì§‘ë‹¨ ìˆ˜ì¤€ì˜ ì§„í™” ë° ìµœì í™”
"""

from __future__ import annotations
import os
import json
import time
import copy
import random
import hashlib
import uuid
from dataclasses import dataclass, field
from typing import List, Dict, Any, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# alpha_elo.pyì—ì„œ í•µì‹¬ ê¸°ëŠ¥ë“¤ import
from alpha_elo import (
    EvolverConfig, Archive, Judge, AnthropicClientWrapper, OpenAIClientWrapper,
    load_text_prompt, get_prompt_manager, make_unique_id, ensure_dir,
    apply_variation_multi_parent, normalize_judge_scores_for_pool,

)

# RAG helpers
from rag_utils import (
    normalize_feedback_keys,
)



@dataclass
class GroupEvolverConfig(EvolverConfig):
    """ì§‘ë‹¨ ì§„í™”ë¥¼ ìœ„í•œ í™•ì¥ëœ ì„¤ì •"""
    
    # í†µí•© ì§‘ë‹¨ í‰ê°€ (ì´ 100ì )
    max_group_score: int = 100
    
    # ì§‘ë‹¨ í¬ê¸° ì„¤ì •
    min_group_size: int = 10
    max_group_size: int = 20
    target_group_size: int = 5
    
    
    # ì§‘ë‹¨ ì§„í™” ì„¤ì •
    group_crossover_rate: float = 0.3  # ê·¸ë£¹ê°„ êµë°° í™•ë¥ 
    group_mutation_rate: float = 0.4   # ê·¸ë£¹ ë‚´ ë³€ì´ í™•ë¥ 

    # RAG ì˜µì…˜
    use_rag: bool = True
    rag_top_k: int = 5


class UnifiedGroupJudge:
    """í†µí•© ì§‘ë‹¨ spec í‰ê°€ë¥¼ ìœ„í•œ Judge í´ë˜ìŠ¤"""
    
    def __init__(self, client, max_points: int = 100):
        self.client = client
        self.max_points = max_points
        
        # í†µí•© í”„ë¡¬í”„íŠ¸ ë¡œë“œ (ì¡°ìš©íˆ)
        prompt_file = "prompts/unified_group_judge_prompt.txt"
        self.prompt_template = load_text_prompt(prompt_file)

        if not self.prompt_template:
            raise FileNotFoundError(f"Unified Group Judge í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_file}")
    
    def score_group(self, spec_group: List[Dict[str, Any]], domain_profile: str,
                   task_profile: str, max_tokens: int = 800) -> Tuple[Dict[str, int], int, str, str, Dict[str, str]]:
        """ì§‘ë‹¨ specë“¤ì„ í†µí•© í‰ê°€í•˜ì—¬ ì„¸ë¶€ ì ìˆ˜ì™€ ì´ì  ë°˜í™˜"""
        
        # ì§‘ë‹¨ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        group_text = self._format_spec_group(spec_group)
        
        prompt = self.prompt_template.format(
            domain_profile=domain_profile,
            task_profile=task_profile,
            spec_group=group_text
        )
        
        try:
            raw = self.client.generate(prompt, max_tokens=max_tokens, temperature=0.1)

            # ì„¸ë¶€ ì ìˆ˜ë“¤ê³¼ comment íŒŒì‹±
            scores = {}
            comments = {}
            total = None

            lines = raw.split('\n')
            i = 0
            while i < len(lines):
                line = lines[i].strip()

                # ì ìˆ˜ íŒŒì‹±
                if line.startswith('COHESION_SCORE='):
                    try:
                        scores['cohesion'] = int(line.split('=')[1].strip())
                        # ë‹¤ìŒ ì¤„ì— commentê°€ ìˆëŠ”ì§€ í™•ì¸
                        if i + 1 < len(lines) and 'Comment:' in lines[i + 1]:
                            comments['cohesion'] = lines[i + 1].split('Comment:', 1)[1].strip()
                            i += 1  # comment ì¤„ë„ ê±´ë„ˆëœ€
                    except:
                        scores['cohesion'] = 0
                elif line.startswith('COVERAGE_SCORE='):
                    try:
                        scores['coverage'] = int(line.split('=')[1].strip())
                        if i + 1 < len(lines) and 'Comment:' in lines[i + 1]:
                            comments['coverage'] = lines[i + 1].split('Comment:', 1)[1].strip()
                            i += 1
                    except:
                        scores['coverage'] = 0
                elif line.startswith('REDUNDANCY_SCORE='):
                    try:
                        scores['redundancy'] = int(line.split('=')[1].strip())
                        if i + 1 < len(lines) and 'Comment:' in lines[i + 1]:
                            comments['redundancy'] = lines[i + 1].split('Comment:', 1)[1].strip()
                            i += 1
                    except:
                        scores['redundancy'] = 0
                elif line.startswith('PRACTICALITY_SCORE='):
                    try:
                        scores['practicality'] = int(line.split('=')[1].strip())
                        if i + 1 < len(lines) and 'Comment:' in lines[i + 1]:
                            comments['practicality'] = lines[i + 1].split('Comment:', 1)[1].strip()
                            i += 1
                    except:
                        scores['practicality'] = 0
                elif line.startswith('TOTAL='):
                    try:
                        total = int(line.split('=')[1].strip())
                    except:
                        pass

                i += 1
            
            # ì´ì ì´ ì—†ìœ¼ë©´ ì„¸ë¶€ ì ìˆ˜ í•©ê³„ë¡œ ê³„ì‚°
            if total is None:
                total = sum(scores.values())
            
            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
            scores['cohesion'] = max(0, min(30, scores.get('cohesion', 0)))
            scores['coverage'] = max(0, min(25, scores.get('coverage', 0)))
            scores['redundancy'] = max(0, min(25, scores.get('redundancy', 0)))
            scores['practicality'] = max(0, min(20, scores.get('practicality', 0)))
            total = max(0, min(self.max_points, total))
            
            return scores, total, prompt, raw, comments
            
        except Exception as e:
            print(f"  âš ï¸ í†µí•© ê·¸ë£¹ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'cohesion': 0, 'coverage': 0, 'redundancy': 0, 'practicality': 0}, 0, "", "", {}
    
    def _format_spec_group(self, spec_group: List[Dict[str, Any]]) -> str:
        """ì§‘ë‹¨ specë“¤ì„ í‰ê°€ìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ… - í‰ê°€ í’ˆì§ˆ í–¥ìƒ"""
        formatted = []
        for i, spec in enumerate(spec_group, 1):
            spec_text = spec.get('text', '').strip()
            spec_id = spec.get('id', f'spec_{i}')

            score = spec.get('score', 'N/A')

            # í‰ê°€ ì •ë³´ í¬í•¨
            formatted.append(f"[SPEC {i}] (ID: {spec_id}, Score: {score})\n{spec_text}\n")

        # ê·¸ë£¹ í†µê³„ ì¶”ê°€
        total_specs = len(spec_group)

        avg_score = sum(float(s.get('score', 0)) for s in spec_group) / total_specs if spec_group else 0

        header = f"GROUP OVERVIEW: {total_specs} specs, Avg Score: {avg_score:.1f}\n"
        header += "="*80 + "\n"

        return header + "\n".join(formatted) + "\n" + "="*80

    def _analyze_group_feedback(self, comments: Dict[str, str]) -> Dict[str, str]:
        """ì›ë³¸ judge ì½”ë©˜íŠ¸ë“¤ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
        if not comments:
            return {
                'cohesion': 'No cohesion feedback available',
                'coverage': 'No coverage feedback available',
                'redundancy': 'No redundancy feedback available',
                'practicality': 'No practicality feedback available'
            }

        return {
            'cohesion': comments.get('cohesion', 'No cohesion feedback'),
            'coverage': comments.get('coverage', 'No coverage feedback'),
            'redundancy': comments.get('redundancy', 'No redundancy feedback'),
            'practicality': comments.get('practicality', 'No practicality feedback')
        }



    def _format_examples_with_feedback(self, groups: List[Dict[str, Any]], tag: str) -> str:
        """ê·¸ë£¹ë“¤ì„ í‰ê°€ í”¼ë“œë°±ê³¼ í•¨ê»˜ í¬ë§·íŒ…"""
        if not groups:
            return f"\n[{tag}] (none)\n"

        formatted = []
        for i, group in enumerate(groups[:3], 1):  # ìµœëŒ€ 3ê°œ ê·¸ë£¹ë§Œ
            specs = group.get('specs', [])
            group_score = group.get('group_score', 0)
            comments = group.get('group_comments', {})

            # ì›ë³¸ ì½”ë©˜íŠ¸ ì‚¬ìš©
            feedback = self._analyze_group_feedback(comments)

            formatted.append(f"[{tag}] Group {i} (Score: {group_score}/100)")
            formatted.append(f"ğŸ’¬ Cohesion: {feedback['cohesion'][:100]}{'...' if len(feedback['cohesion']) > 100 else ''}")
            formatted.append(f"ğŸ’¬ Coverage: {feedback['coverage'][:100]}{'...' if len(feedback['coverage']) > 100 else ''}")
            formatted.append("")

            # ëŒ€í‘œ specë“¤ (ìµœëŒ€ 2ê°œ)
            for j, spec in enumerate(specs[:2], 1):
                spec_text = spec.get('text', '').strip()
                spec_score = spec.get('score', 'N/A')
                formatted.append(f"  â€¢ Spec {j} (Score: {spec_score}): {spec_text[:120]}...")

            formatted.append("")

        return "\n".join(formatted)


class GroupArchive:
    """ì§‘ë‹¨ specë“¤ì„ ê´€ë¦¬í•˜ëŠ” Archive"""
    
    def __init__(self, max_capacity: int = 50):
        self.max_capacity = max_capacity
        self.groups: List[Dict[str, Any]] = []  # ê° ê·¸ë£¹ì€ specs ë¦¬ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° í¬í•¨
    
    def add_group(self, group: Dict[str, Any]):
        """ìƒˆë¡œìš´ ì§‘ë‹¨ì„ ì¶”ê°€"""
        group.setdefault('id', make_unique_id(str(group.get('specs', []))))
        
        # ê¸°ì¡´ ê·¸ë£¹ê³¼ ID ì¤‘ë³µ í™•ì¸
        existing_ids = {g.get('id') for g in self.groups}
        if group['id'] in existing_ids:
            for i, g in enumerate(self.groups):
                if g['id'] == group['id']:
                    # ê¸°ì¡´ ê·¸ë£¹ ì—…ë°ì´íŠ¸
                    self.groups[i] = group
                    break
        else:
            self.groups.append(group)
        
        # ê·¸ë£¹ ì ìˆ˜ ìš°ì„  ì •ë ¬
        self.groups.sort(key=lambda x: x.get('group_score', 0), reverse=True)
        
        # ìš©ëŸ‰ ì´ˆê³¼ ì‹œ ì œê±°
        if len(self.groups) > self.max_capacity:
            self.groups = self.groups[:self.max_capacity]
    
    def sample_parent_groups(self, n: int) -> List[Dict[str, Any]]:
        """ë¶€ëª¨ ì§‘ë‹¨ ìƒ˜í”Œë§"""
        if not self.groups:
            return []
        
        # ìƒìœ„ 30ê°œ ì¤‘ì—ì„œ ì¤‘ë³µ í—ˆìš© ìƒ˜í”Œë§
        top_groups = self.groups[:min(30, len(self.groups))]
        return random.choices(top_groups, k=min(n, len(top_groups)))
    
    def all_groups(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì§‘ë‹¨ ë°˜í™˜"""
        return self.groups.copy()


def create_spec_groups_from_clustering(specs: List[Dict[str, Any]], 
                                     cfg: GroupEvolverConfig) -> List[List[Dict[str, Any]]]:
    """í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì˜ë¯¸ì ìœ¼ë¡œ coherentí•œ spec ê·¸ë£¹ë“¤ì„ ìƒì„±"""
    
    if len(specs) < cfg.min_group_size:
        return [specs]  # ë„ˆë¬´ ì ìœ¼ë©´ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ
    
    try:
        # ì¤‘ë³µ í—ˆìš© ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ê·¸ë£¹ ìƒì„± (15-20ê°œì”©)
        import random

        # ê·¸ë£¹ í¬ê¸° ì„¤ì • (15-20ê°œì”©)
        group_size = random.randint(15, 20)

        # ê·¸ë£¹ ê°œìˆ˜ ê³„ì‚° (specs ê¸¸ì´ ê¸°ë°˜)
        num_groups = max(1, len(specs) // group_size)
        if len(specs) % group_size > 0:
            num_groups += 1

        groups = []
        cluster_descriptions = []

        for i in range(num_groups):
            # ì¤‘ë³µ í—ˆìš©í•˜ì—¬ ëœë¤ ì„ íƒ (ë³µì› ì¶”ì¶œ)
            group_specs = random.choices(specs, k=min(group_size, len(specs) * 2))  # ìµœëŒ€ 2ë°°ê¹Œì§€ í—ˆìš©
            
            if group_specs:  # ë¹ˆ ê·¸ë£¹ì´ ì•„ë‹ˆë©´
                groups.append(group_specs)
                cluster_descriptions.append(f"ëœë¤ ê·¸ë£¹ {i+1} (í¬ê¸°: {len(group_specs)}, ì¤‘ë³µí—ˆìš©)")

        print(f"  ğŸ² ì¤‘ë³µ í—ˆìš© ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ {len(groups)}ê°œ ê·¸ë£¹ ìƒì„± (í‰ê·  í¬ê¸°: {sum(len(g) for g in groups)/len(groups):.1f})")
        return groups
        
    except Exception as e:
        print(f"  âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ê·¸ë£¹ ìƒì„± ì‹¤íŒ¨: {e}")
        # Fallback: ìˆœì°¨ì  ê·¸ë£¹ ë¶„í• 
        groups = []
        for i in range(0, len(specs), cfg.target_group_size):
            group = specs[i:i + cfg.target_group_size]
            if len(group) >= cfg.min_group_size:
                groups.append(group)
        return groups


def evaluate_spec_group(group_specs: List[Dict[str, Any]],
                       unified_judge: UnifiedGroupJudge,
                       domain_profile: str,
                       task_profile: str,
                       generation: int = 0,
                       judges_log_dir: str = None) -> Dict[str, Any]:
    """í†µí•© judgeë¡œ ì§‘ë‹¨ specë“¤ì„ í‰ê°€"""
    
    if not group_specs:
        return {
            'specs': group_specs,
            'group_scores': {'cohesion': 0, 'coverage': 0, 'redundancy': 0, 'practicality': 0},
            'group_score': 0,
            'evaluated_at': time.time(),


            'id': make_unique_id('empty_group')
        }
    
    try:
        # í†µí•© í‰ê°€ ìˆ˜í–‰
        group_scores, total_score, prompt, raw_response, comments = unified_judge.score_group(
            group_specs, domain_profile, task_profile
        )
        
        # ë¡œê·¸ ì €ì¥
        if judges_log_dir:
            timestamp = int(time.time() * 1000)
            group_id = make_unique_id(str([s.get('id') for s in group_specs]))
            log_file = os.path.join(judges_log_dir, f"gen{generation:03d}_group_{group_id}_unified.json")
            
            log_data = {
                'generation': generation,
                'timestamp': timestamp,
                'group_id': group_id,
                'group_size': len(group_specs),
                'spec_ids': [s.get('id') for s in group_specs],
                'unified_judge': {
                    'prompt': prompt,
                    'raw_response': raw_response,
                    'parsed_scores': group_scores,
                    'comments': comments,
                    'total_score': total_score,
                    'temperature': 0.1,
                    'max_tokens': 800
                },
                'final_group_scores': group_scores,
                'total_group_score': total_score
            }
            
            try:
                with open(log_file, 'w', encoding='utf-8') as f:
                    json.dump(log_data, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"Warning: Unified group judge ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨ {log_file}: {e}")
        
        return {
            'specs': group_specs,
            'group_scores': group_scores,
            'group_score': total_score,
            'group_comments': comments,
            'evaluated_at': time.time(),


            'id': make_unique_id(str([s.get('id') for s in group_specs]))
        }
        
    except Exception as e:
        print(f"  âš ï¸ í†µí•© ê·¸ë£¹ í‰ê°€ ì‹¤íŒ¨: {e}")
        return {
            'specs': group_specs,
            'group_scores': {'cohesion': 0, 'coverage': 0, 'redundancy': 0, 'practicality': 0},
            'group_score': 0,
            'group_comments': {},
            'evaluated_at': time.time(),


            'id': make_unique_id(str([s.get('id') for s in group_specs]))
        }


def group_crossover(parent_groups: List[Dict[str, Any]],
                   cfg: GroupEvolverConfig) -> List[Dict[str, Any]]:
    """ì§‘ë‹¨ ê°„ êµë°° (spec êµí™˜) - ì „ëµì  ê°œì„ """

    if len(parent_groups) < 2:
        return parent_groups

    offspring_groups = []

    for i in range(0, len(parent_groups), 2):
        parent1 = parent_groups[i]
        parent2 = parent_groups[i + 1] if i + 1 < len(parent_groups) else parent_groups[0]

        if random.random() < cfg.group_crossover_rate:
            # ì „ëµì  êµë°° ìˆ˜í–‰
            specs1 = parent1['specs'].copy()
            specs2 = parent2['specs'].copy()

            # 1. ê° ê·¸ë£¹ì˜ ê°•ì /ì•½ì  ë¶„ì„
            score1 = parent1.get('group_score', 0)
            score2 = parent2.get('group_score', 0)

            # 2. êµí™˜í•  spec ê°œìˆ˜ ê²°ì • (ë” ë‚®ì€ ì ìˆ˜ì˜ ê·¸ë£¹ì´ ë” ë§ì´ êµí™˜)
            base_exchange = min(len(specs1), len(specs2)) // 4
            score_diff = abs(score1 - score2)
            exchange_bonus = int(score_diff / 20)  # ì ìˆ˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ë” ë§ì´ êµí™˜
            exchange_count = min(base_exchange + exchange_bonus, min(len(specs1), len(specs2)) // 2)

            if exchange_count > 0:
                # 3. ì „ëµì  êµí™˜: ë‚®ì€ ì ìˆ˜ì˜ ê·¸ë£¹ì—ì„œ ë” ì¢‹ì€ specì„ ê°€ì ¸ì˜´
                if score1 < score2:
                    # parent1ì´ ë‚®ì€ ì ìˆ˜ì´ë¯€ë¡œ parent2ì—ì„œ ì¢‹ì€ spec ê°€ì ¸ì˜¤ê¸°
                    specs2_sorted = sorted(specs2, key=lambda s: float(s.get('score', 0)), reverse=True)
                    to_remove2 = specs2_sorted[:exchange_count]  # parent2ì˜ ìµœê³  specë“¤

                    specs1_sorted = sorted(specs1, key=lambda s: float(s.get('score', 0)))
                    to_remove1 = specs1_sorted[:exchange_count]  # parent1ì˜ ìµœì € specë“¤
                else:
                    # parent2ê°€ ë‚®ì€ ì ìˆ˜ì´ë¯€ë¡œ parent1ì—ì„œ ì¢‹ì€ spec ê°€ì ¸ì˜¤ê¸°
                    specs1_sorted = sorted(specs1, key=lambda s: float(s.get('score', 0)), reverse=True)
                    to_remove1 = specs1_sorted[:exchange_count]  # parent1ì˜ ìµœê³  specë“¤

                    specs2_sorted = sorted(specs2, key=lambda s: float(s.get('score', 0)))
                    to_remove2 = specs2_sorted[:exchange_count]  # parent2ì˜ ìµœì € specë“¤

                # 4. êµí™˜ ìˆ˜í–‰
                new_specs1 = [s for s in specs1 if s not in to_remove1] + to_remove2
                new_specs2 = [s for s in specs2 if s not in to_remove2] + to_remove1

                # 5. í¬ê¸° ì¡°ì •
                new_specs1 = new_specs1[:cfg.max_group_size]
                new_specs2 = new_specs2[:cfg.max_group_size]

                offspring_groups.append({
                    'specs': new_specs1,
                    'meta': {'origin': 'crossover', 'strategy': 'selective_exchange',
                            'parents': [parent1.get('id'), parent2.get('id')]}
                })
                offspring_groups.append({
                    'specs': new_specs2,
                    'meta': {'origin': 'crossover', 'strategy': 'selective_exchange',
                            'parents': [parent1.get('id'), parent2.get('id')]}
                })
            else:
                offspring_groups.extend([parent1, parent2])
        else:
            offspring_groups.extend([parent1, parent2])

    return offspring_groups


def deduplicate_specs_with_llm(specs: List[Dict[str, Any]], client, domain_profile: str, task_profile: str) -> List[Dict[str, Any]]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µëœ specë“¤ì„ ì œê±°
    ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•˜ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” specì„ í†µí•©
    """
    if not specs:
        return []
    
    # Spec í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    spec_texts = [spec.get('text', '') for spec in specs]
    
    # LLMì—ê²Œ ì¤‘ë³µ ì œê±° ìš”ì²­
    dedup_prompt = f"""You are a specification deduplication expert.

[Context]
{domain_profile}
{task_profile}

[Task]
Remove duplicate or highly redundant specifications from the following list.
Keep only unique, non-overlapping specifications.

Guidelines:
1. Remove specs that are semantically identical (even if worded differently)
2. If two specs cover 80%+ of the same content, keep only the more comprehensive one
3. Keep specs that address different aspects or add unique value
4. Maintain RFC2119 keywords (MUST/SHOULD/MAY/MUST NOT/SHOULD NOT)
5. Preserve the exact wording of kept specs (do not rewrite)

[Input Specifications]
{chr(10).join([f"{i+1}. {text}" for i, text in enumerate(spec_texts)])}

[Output Format]
Return ONLY the numbers of specifications to KEEP, separated by commas.
Example: 1,3,5,7,9,12,15

Numbers to keep:"""

    try:
        response = client.generate(dedup_prompt, max_tokens=500, temperature=0.1)
        
        # ì‘ë‹µ íŒŒì‹±: ìˆ«ìë“¤ë§Œ ì¶”ì¶œ
        import re
        numbers = re.findall(r'\d+', response)
        keep_indices = [int(n) - 1 for n in numbers if 0 < int(n) <= len(specs)]  # 1-based to 0-based
        
        # ìœ íš¨ì„± ê²€ì‚¬
        if not keep_indices:
            print("  âš ï¸ LLM ì¤‘ë³µ ì œê±° ì‹¤íŒ¨, ëª¨ë“  spec ìœ ì§€")
            return specs
        
        # ì„ íƒëœ specë§Œ ë°˜í™˜
        deduplicated = [specs[i] for i in keep_indices if i < len(specs)]
        
        print(f"  ğŸ” ì¤‘ë³µ ì œê±°: {len(specs)}ê°œ â†’ {len(deduplicated)}ê°œ")
        return deduplicated
        
    except Exception as e:
        print(f"  âš ï¸ ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜: {e}, ì›ë³¸ ë°˜í™˜")
        return specs


def extract_forbidden_topics(specs: List[Dict[str, Any]], top_n: int = 8) -> str:
    """
    ê¸°ì¡´ specë“¤ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ì£¼ì œ/í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ê¸ˆì§€ ëª©ë¡ ìƒì„±
    ë„ë©”ì¸ ë¬´ê´€í•˜ê²Œ ì‘ë™í•˜ëŠ” ìë™í™” í•¨ìˆ˜
    """
    from collections import Counter
    import re
    
    # ëª¨ë“  spec í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_text = " ".join([spec.get('text', '') for spec in specs])
    
    # ì£¼ìš” ëª…ì‚¬êµ¬/ê°œë… ì¶”ì¶œ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´, 2-4 ë‹¨ì–´ ì—°ì†)
    # ì˜ˆ: "pharmaceutical compound synthesis", "DEA numbers", "prompt injection"
    patterns = [
        r'\b[A-Z][a-z]+(?:\s+[a-z]+){1,3}\b',  # ëŒ€ë¬¸ì ì‹œì‘ êµ¬ë¬¸
        r'\b(?:MUST|SHOULD|MAY)\s+(?:NOT\s+)?[a-z]+\s+[a-z]+(?:\s+[a-z]+){0,3}',  # ê·œì¹™ íŒ¨í„´
    ]
    
    phrases = []
    for pattern in patterns:
        phrases.extend(re.findall(pattern, all_text))
    
    # ë¹ˆë„ ê³„ì‚°
    phrase_counts = Counter(phrases)
    
    # ìƒìœ„ Nê°œ ì„ íƒ
    top_phrases = [phrase for phrase, count in phrase_counts.most_common(top_n) if count >= 2]
    
    # ì¶”ê°€: ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì¼ í‚¤ì›Œë“œ ì¶”ì¶œ (4ê¸€ì ì´ìƒ, ì¼ë°˜ì ì´ì§€ ì•Šì€ ë‹¨ì–´)
    words = re.findall(r'\b[a-z]{4,}\b', all_text.lower())
    common_words = {'must', 'should', 'generate', 'detect', 'include', 'provide', 'ensure', 'verify', 'maintain', 'with', 'from', 'that', 'this', 'when', 'while', 'before', 'after'}
    word_counts = Counter([w for w in words if w not in common_words])
    top_words = [word for word, count in word_counts.most_common(top_n) if count >= 3]
    
    # í¬ë§·íŒ…
    forbidden_list = []
    
    if top_phrases:
        forbidden_list.append("ğŸ“Œ Overused phrases/concepts:")
        for phrase in top_phrases[:5]:
            forbidden_list.append(f"  - {phrase}")
    
    if top_words:
        forbidden_list.append("ğŸ“Œ Overused keywords:")
        forbidden_list.append(f"  - {', '.join(top_words[:10])}")
    
    if not forbidden_list:
        return "None identified - encourage diverse coverage."
    
    return "\n".join(forbidden_list)


def group_mutation(groups: List[Dict[str, Any]],
                  generator,
                  unified_judge,
                  best_groups_history: List[Dict[str, Any]],
                  worst_groups_history: List[Dict[str, Any]],
                  constitution: str,
                  domain_profile: str,
                  task_profile: str,
                  cfg: GroupEvolverConfig,
                  generation: int = 0,
                  generator_log_dir: str = None,
                  domain_name: str = None,
                  task_name: str = None,
                  single_spec_pool: Optional[List[Dict[str, Any]]] = None,
                  rag_log_dir: Optional[str] = None) -> List[Dict[str, Any]]:
    """ì§‘ë‹¨ ë‚´ ë³€ì´ (ìƒˆë¡œìš´ spec ìƒì„± ë˜ëŠ” ê¸°ì¡´ spec ê°œì„ ) - ì „ëµì  ê°œì„ """

    mutated_groups = []

    for group in groups:
        if random.random() < cfg.group_mutation_rate:
            # ë³€ì´ ìˆ˜í–‰
            specs = group['specs'].copy()
            group_score = group.get('group_score', 0)

            # í‰ê°€ í”¼ë“œë°± ê¸°ë°˜ good/bad examples í™œìš©
            if hasattr(unified_judge, '_format_examples_with_feedback'):
                good_examples_text = unified_judge._format_examples_with_feedback(best_groups_history, "SUCCESSFUL")
                bad_examples_text = unified_judge._format_examples_with_feedback(worst_groups_history, "IMPROVEMENT_NEEDED")
            else:
                good_examples_text = "Successful groups: High cohesion and coverage"
                bad_examples_text = "Areas for improvement: Better redundancy and practicality"

            # ì „ëµì  ë³€ì´ ì„ íƒ
            if len(specs) < cfg.max_group_size and (group_score < 70 or random.random() < 0.7):
                # 1. ê·¸ë£¹ ì ìˆ˜ê°€ ë‚®ê±°ë‚˜ í™•ë¥ ì ìœ¼ë¡œ ìƒˆ spec ì¶”ê°€ (ê·¸ë£¹ ë‹¤ì–‘ì„± ì¦ëŒ€)
                try:
                    # ê·¸ë£¹ ë‚´ ìµœê³  ì„±ëŠ¥ specë“¤ì„ ë¶€ëª¨ë¡œ ì„ íƒ
                    parent_specs = sorted(specs, key=lambda s: float(s.get('score', 0)), reverse=True)[:3]
                    if len(parent_specs) < 2:
                        parent_specs = specs[:min(3, len(specs))]

                    # ê·¸ë£¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ + í‰ê°€ í”¼ë“œë°± í™œìš©
                    if hasattr(unified_judge, '_analyze_group_feedback'):
                        feedback = unified_judge._analyze_group_feedback(group.get('group_comments', {}))
                    else:
                        feedback = {'cohesion': 'Good group structure', 'coverage': 'Good coverage', 'redundancy': 'Balanced redundancy', 'practicality': 'Practical implementation'}
                    group_context = f"Group Context: This is part of a {len(specs)}-spec group. " \
                                  f"Average group score: {group_score:.1f}/100. " \
                                  f"Cohesion: {feedback['cohesion']}. " \
                                  f"Coverage: {feedback['coverage']}. " \
                                  f"Redundancy: {feedback['redundancy']}. " \
                                  f"Practicality: {feedback['practicality']}."

                    # í‰ê°€ í”¼ë“œë°± ê¸°ë°˜ examples ì¶”ê°€ + RAG ë³´ê°•
                    learning_context = f"\n\nLEARNING FROM PAST GROUPS:\n{good_examples_text}\n{bad_examples_text}"

                    # ê°„ë‹¨í•œ í”¼ë“œë°± ì§ì ‘ ì „ë‹¬ (RAG ëŒ€ì‹ )
                    if getattr(cfg, 'use_rag', False) and single_spec_pool:
                        try:
                            # í”¼ë“œë°±ì„ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                            feedback_text = "\n".join([
                                f"â€¢ Cohesion: {feedback.get('cohesion', 'N/A')}",
                                f"â€¢ Coverage: {feedback.get('coverage', 'N/A')}",
                                f"â€¢ Redundancy: {feedback.get('redundancy', 'N/A')}",
                                f"â€¢ Practicality: {feedback.get('practicality', 'N/A')}"
                            ])

                            # ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ specë“¤ ëª‡ ê°œë§Œ ê³¨ë¼ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                            similar_specs = []
                            if single_spec_pool:
                                try:
                                    # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚°: í”¼ë“œë°± í‚¤ì›Œë“œì™€ spec í…ìŠ¤íŠ¸ ê°„ í† í° ê²¹ì¹¨
                                    feedback_keywords = set()
                                    for comment in feedback.values():
                                        if isinstance(comment, str) and comment and comment != 'N/A':
                                            words = [w.lower() for w in comment.split() if len(w) > 3]
                                            feedback_keywords.update(words)

                                    spec_scores = []
                                    for spec in single_spec_pool:
                                        if isinstance(spec, dict):
                                            spec_text = spec.get('text', '').lower()
                                            score = sum(1 for word in feedback_keywords if word in spec_text)
                                            spec_scores.append((score, spec))

                                    # ìƒìœ„ 3ê°œ ì„ íƒ (ì•ˆì „í•˜ê²Œ)
                                    if spec_scores:
                                        spec_scores.sort(key=lambda x: x[0], reverse=True)
                                        similar_specs = [spec for score, spec in spec_scores[:3] if score > 0]

                                except Exception as sort_error:
                                    print(f"  âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì •ë ¬ ì˜¤ë¥˜: {sort_error}")
                                    # fallback: ëœë¤í•˜ê²Œ 3ê°œ ì„ íƒ
                                    similar_specs = single_spec_pool[:3]

                            # ë¡œê·¸ ì €ì¥ (ê°„ë‹¨ ë²„ì „)
                            if rag_log_dir:
                                os.makedirs(rag_log_dir, exist_ok=True)
                                rag_record = {
                                    'generation': generation,
                                    'group_id': group.get('id'),
                                    'strategy': 'add_new_spec',
                                    'feedback': feedback,
                                    'similar_specs_count': len(similar_specs),
                                    'selected_specs': [
                                        {
                                            'id': s.get('id'),
                                            'score': s.get('score', 0),
                                            'elo': s.get('elo'),
                                            'text': s.get('text', '')[:200]
                                        } for s in similar_specs
                                    ]
                                }
                                fname = os.path.join(
                                    rag_log_dir, f"gen{generation:03d}_{group.get('id','group')}_rag.json"
                                )
                                with open(fname, 'w', encoding='utf-8') as rf:
                                    json.dump(rag_record, rf, ensure_ascii=False, indent=2)

                            # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ì— í”¼ë“œë°±ê³¼ ìœ ì‚¬ specë“¤ ì¶”ê°€
                            if similar_specs:
                                feedback_block = f"\nCURRENT GROUP FEEDBACK:\n{feedback_text}"
                                refs_lines = []
                                for idx, s in enumerate(similar_specs, 1):
                                    snippet = s.get('text', '').strip().replace('\n', ' ')
                                    if len(snippet) > 100:
                                        snippet = snippet[:100] + '...'
                                    refs_lines.append(f"  - [{idx}] (Score: {s.get('score','N/A')}) {snippet}")

                                refs_block = "\nSIMILAR EXISTING SPECS:\n" + "\n".join(refs_lines)
                                learning_context += f"\n\n{feedback_block}{refs_block}"

                        except Exception as _e:
                            print(f"  âš ï¸ í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {_e}")

                    # ğŸ”¥ ë‹¤ì–‘ì„± ê°•ì œ: í˜„ì¬ ê·¸ë£¹ì—ì„œ ìì£¼ ë‚˜ì˜¨ ì£¼ì œ ì¶”ì¶œ
                    forbidden_topics_str = extract_forbidden_topics(specs, top_n=8)
                    
                    new_specs = apply_variation_multi_parent(
                        parent_specs,
                        generator, constitution, domain_profile,
                        task_profile + "\n\n" + group_context + learning_context,
                        generation, generator_log_dir, domain_name, task_name,
                        forbidden_topics=forbidden_topics_str
                    )
                    if new_specs:
                        # ìƒì„±ëœ spec ì¤‘ ìµœê³  ì„±ëŠ¥ í•˜ë‚˜ë§Œ ì¶”ê°€
                        best_new_spec = max(new_specs, key=lambda s: float(s.get('score', 0)))
                        specs.append(best_new_spec)
                        print(f"  â• ê·¸ë£¹ ë³€ì´: ìƒˆ spec ì¶”ê°€ (ì ìˆ˜: {best_new_spec.get('score', 0)})")
                except Exception as e:
                    print(f"  âš ï¸ ê·¸ë£¹ ë³€ì´ ì¤‘ ìƒˆ spec ìƒì„± ì‹¤íŒ¨: {e}")

            elif specs and (group_score > 80 or random.random() < 0.5):
                # 2. ê·¸ë£¹ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ê¸°ì¡´ spec ê°œì„  (ê·¸ë£¹ í’ˆì§ˆ í–¥ìƒ)
                try:
                    # ê·¸ë£¹ ë‚´ ìµœì € ì„±ëŠ¥ specì„ ê°œì„  ëŒ€ìƒìœ¼ë¡œ ì„ íƒ
                    target_spec = min(specs, key=lambda s: float(s.get('score', 0)))
                    other_specs = [s for s in specs if s != target_spec]

                    # ê°œì„ ì„ ìœ„í•œ ë¶€ëª¨ ì„ íƒ: target + ê·¸ë£¹ ë‚´ ìµœê³  ì„±ëŠ¥ specë“¤
                    improvement_parents = [target_spec]
                    if other_specs:
                        best_others = sorted(other_specs, key=lambda s: float(s.get('score', 0)), reverse=True)
                        improvement_parents.extend(best_others[:2])

                    # ê·¸ë£¹ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ + í‰ê°€ í”¼ë“œë°± í™œìš©
                    if hasattr(unified_judge, '_analyze_group_feedback'):
                        feedback = unified_judge._analyze_group_feedback(group.get('group_comments', {}))
                    else:
                        feedback = {'cohesion': 'Good group structure', 'coverage': 'Good coverage', 'redundancy': 'Balanced redundancy', 'practicality': 'Practical implementation'}
                    improvement_context = f"Group Context: Improving a spec in a {len(specs)}-spec group. " \
                                        f"Average group score: {group_score:.1f}/100. " \
                                        f"Cohesion: {feedback['cohesion']}. " \
                                        f"Coverage: {feedback['coverage']}. " \
                                        f"Target Spec: {target_spec.get('text', '')[:100]}... " \
                                        f"Focus on enhancing this spec while maintaining group cohesion."

                    # í‰ê°€ í”¼ë“œë°± ê¸°ë°˜ examples ì¶”ê°€ + RAG ë³´ê°•
                    learning_context = f"\n\nLEARNING FROM PAST GROUPS:\n{good_examples_text}\n{bad_examples_text}"

                    if getattr(cfg, 'use_rag', False) and single_spec_pool:
                        try:
                            # í”¼ë“œë°±ì„ ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                            feedback_text = "\n".join([
                                f"â€¢ Cohesion: {feedback.get('cohesion', 'N/A')}",
                                f"â€¢ Coverage: {feedback.get('coverage', 'N/A')}",
                                f"â€¢ Redundancy: {feedback.get('redundancy', 'N/A')}",
                                f"â€¢ Practicality: {feedback.get('practicality', 'N/A')}"
                            ])

                            # ê°œì„  ëŒ€ìƒ specê³¼ ìœ ì‚¬í•œ ê¸°ì¡´ specë“¤ ì°¾ì•„ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                            similar_specs = []
                            if single_spec_pool:
                                try:
                                    target_text = target_spec.get('text', '').lower()
                                    # target specì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
                                    target_words = [w.lower() for w in target_text.split() if len(w) > 3]

                                    # í”¼ë“œë°± í‚¤ì›Œë“œ ì¶”ì¶œ
                                    feedback_keywords = set()
                                    for comment in feedback.values():
                                        if isinstance(comment, str) and comment and comment != 'N/A':
                                            words = [w.lower() for w in comment.split() if len(w) > 3]
                                            feedback_keywords.update(words)

                                    spec_scores = []
                                    for spec in single_spec_pool:
                                        if isinstance(spec, dict):
                                            spec_text = spec.get('text', '').lower()
                                            # target specê³¼ì˜ ìœ ì‚¬ë„ + í”¼ë“œë°± í‚¤ì›Œë“œ ìœ ì‚¬ë„
                                            target_score = sum(1 for word in target_words if word in spec_text)
                                            feedback_score = sum(1 for word in feedback_keywords if word in spec_text)
                                            total_score = target_score + feedback_score
                                            spec_scores.append((total_score, spec))

                                    # ìƒìœ„ 3ê°œ ì„ íƒ (ì•ˆì „í•˜ê²Œ)
                                    if spec_scores:
                                        spec_scores.sort(key=lambda x: x[0], reverse=True)
                                        similar_specs = [spec for score, spec in spec_scores[:3] if score > 0]

                                except Exception as sort_error:
                                    print(f"  âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì •ë ¬ ì˜¤ë¥˜: {sort_error}")
                                    # fallback: ëœë¤í•˜ê²Œ 3ê°œ ì„ íƒ
                                    similar_specs = single_spec_pool[:3]

                            # ë¡œê·¸ ì €ì¥ (ê°„ë‹¨ ë²„ì „)
                            if rag_log_dir:
                                os.makedirs(rag_log_dir, exist_ok=True)
                                rag_record = {
                                    'generation': generation,
                                    'group_id': group.get('id'),
                                    'strategy': 'improve_spec',
                                    'target_spec_id': target_spec.get('id'),
                                    'feedback': feedback,
                                    'similar_specs_count': len(similar_specs),
                                    'selected_specs': [
                                        {
                                            'id': s.get('id'),
                                            'score': s.get('score', 0),
                                            'elo': s.get('elo'),
                                            'text': s.get('text', '')[:200]
                                        } for s in similar_specs
                                    ]
                                }
                                fname = os.path.join(
                                    rag_log_dir, f"gen{generation:03d}_{group.get('id','group')}_improve_rag.json"
                                )
                                with open(fname, 'w', encoding='utf-8') as rf:
                                    json.dump(rag_record, rf, ensure_ascii=False, indent=2)

                            # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ì— í”¼ë“œë°±ê³¼ ìœ ì‚¬ specë“¤ ì¶”ê°€
                            if similar_specs:
                                feedback_block = f"\nCURRENT GROUP FEEDBACK:\n{feedback_text}"
                                refs_lines = []
                                for idx, s in enumerate(similar_specs, 1):
                                    snippet = s.get('text', '').strip().replace('\n', ' ')
                                    if len(snippet) > 100:
                                        snippet = snippet[:100] + '...'
                                    refs_lines.append(f"  - [{idx}] (Score: {s.get('score','N/A')}) {snippet}")

                                refs_block = "\nSIMILAR EXISTING SPECS:\n" + "\n".join(refs_lines)
                                learning_context += f"\n\n{feedback_block}{refs_block}"

                        except Exception as _e:
                            print(f"  âš ï¸ í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {_e}")

                    # ğŸ”¥ ë‹¤ì–‘ì„± ê°•ì œ: í˜„ì¬ ê·¸ë£¹ì—ì„œ ìì£¼ ë‚˜ì˜¨ ì£¼ì œ ì¶”ì¶œ
                    forbidden_topics_str = extract_forbidden_topics(specs, top_n=8)
                    
                    improved_specs = apply_variation_multi_parent(
                        improvement_parents,
                        generator, constitution, domain_profile,
                        task_profile + "\n\n" + improvement_context + learning_context,
                        generation, generator_log_dir, domain_name, task_name,
                        forbidden_topics=forbidden_topics_str
                    )

                    if improved_specs:
                        # ê°œì„ ëœ spec ì¤‘ ìµœê³  ì„±ëŠ¥ìœ¼ë¡œ êµì²´
                        best_improved = max(improved_specs, key=lambda s: float(s.get('score', 0)))
                        specs = [s if s != target_spec else best_improved for s in specs]
                        print(f"  ğŸ”„ ê·¸ë£¹ ë³€ì´: spec ê°œì„  ({target_spec.get('score', 0)} â†’ {best_improved.get('score', 0)})")
                except Exception as e:
                    print(f"  âš ï¸ ê·¸ë£¹ ë³€ì´ ì¤‘ spec ê°œì„  ì‹¤íŒ¨: {e}")

            # í¬ê¸° ì¡°ì •
            specs = specs[:cfg.max_group_size]

            mutated_groups.append({
                'specs': specs,
                'meta': {'origin': 'mutation', 'strategy': 'adaptive_improvement',
                        'parent': group.get('id'), 'parent_score': group_score}
            })
        else:
            mutated_groups.append(group)

    return mutated_groups


def run_group_evolution_from_archive(archive: Archive,
                                   task_name: str,
                                   constitution: str,
                                   domain_profile: str,
                                   task_profile: str,
                                   cfg: GroupEvolverConfig,
                                   unified_judge: UnifiedGroupJudge,
                                   base_output_dir: str = None,
                                   domain_name: str = None) -> GroupArchive:
    """ê¸°ì¡´ ê°œë³„ spec archiveë¡œë¶€í„° ì§‘ë‹¨ ê¸°ë°˜ ì •ì±… ì…‹ ìƒì„± ë° ì§„í™”

    ëª©í‘œ:
    1. ê°œë³„ í‰ê°€ë¥¼ í†µí•´ ì„ ì •ëœ specë“¤ì„ ì¡°í•©í•˜ì—¬ ë°©ëŒ€í•œ risk ë²”ìœ„ë¥¼ ì»¤ë²„í•˜ëŠ” ì •ì±… ì…‹ ìƒì„±
    2. ì˜ë¯¸ì ìœ¼ë¡œ coherentí•œ ê·¸ë£¹ í˜•ì„±
    3. ê·¸ë£¹ ë‹¨ìœ„ í‰ê°€ ë° ì§„í™”ë¡œ ìµœì ì˜ ì •ì±… ì¡°í•© íƒìƒ‰
    """

    # ê¸°ì¡´ archiveì—ì„œ ìƒìœ„ specë“¤ ì¶”ì¶œ
    all_elite_specs = archive.all_elites()
    print(f"ğŸš€ ê¸°ì¡´ archiveì—ì„œ ì§‘ë‹¨ ê¸°ë°˜ ì •ì±… ì…‹ ìƒì„±: {task_name}")
    print(f"  ğŸ“Š ê¸°ì¡´ archive í¬ê¸°: {len(all_elite_specs)}ê°œ spec")
    print(f"  ğŸ¯ ëª©í‘œ ê·¸ë£¹ í¬ê¸°: {cfg.min_group_size}-{cfg.max_group_size} (target: {cfg.target_group_size})")
    print(f"  ğŸ“ˆ ìµœê³  ì ìˆ˜: {all_elite_specs[0].get('score', 0)}")

    # ìƒìœ„ specë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° ê·¸ë£¹ ìƒì„±
    top_specs = all_elite_specs[:min(100, len(all_elite_specs))]  # ìƒìœ„ 100ê°œ ì‚¬ìš©
    
    # ëª¨ë¸ í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (GeneratorëŠ” Claude Sonnet ì‚¬ìš©)
    if not cfg.anthropic_api_key:
        raise ValueError("âŒ Generatorìš© ANTHROPIC_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    client_gen = AnthropicClientWrapper(api_key=cfg.anthropic_api_key, model="claude-sonnet-4-20250514")
    
    # unified_judgeëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë°›ìŒ (ì¤‘ë³µ ìƒì„± ë°©ì§€)

    # ìµœê³ /ìµœì € ì„±ëŠ¥ ê·¸ë£¹ë“¤ì„ ì¶”ì í•˜ì—¬ good/bad examples ìƒì„±ìš©
    best_groups_history = []
    worst_groups_history = []
    
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì„¤ì •
    if base_output_dir is None:
        base_output_dir = cfg.output_dir
    
    if cfg.use_timestamp_suffix:
        timestamp_suffix = time.strftime("_%Y%m%d_%H%M%S")
        base_output_dir = base_output_dir + timestamp_suffix
    
    if domain_name:
        safe_domain = domain_name.replace(' & ', '_and_').replace(' ', '_')
        safe_task = task_name.replace(' & ', '_and_').replace(' ', '_')
        out_dir = os.path.join(base_output_dir, safe_domain, safe_task)
    else:
        out_dir = os.path.join(base_output_dir, task_name.replace(' ', '_'))
    
    ensure_dir(out_dir)
    best_dir = os.path.join(out_dir, 'best_groups')
    generator_dir = os.path.join(out_dir, 'generator')
    judges_dir = os.path.join(out_dir, 'group_judges')
    rag_dir = os.path.join(out_dir, 'rag')
    ensure_dir(best_dir); ensure_dir(generator_dir); ensure_dir(judges_dir); ensure_dir(rag_dir)
    
    # ì§‘ë‹¨ Archive ì´ˆê¸°í™”
    group_archive = GroupArchive(max_capacity=50)
    
    # ì´ˆê¸° ì§‘ë‹¨ ìƒì„± (ê¸°ì¡´ archiveì˜ ìƒìœ„ specë“¤ë¡œë¶€í„°)
    print(f"\nğŸ§¬ ê¸°ì¡´ archiveë¡œë¶€í„° ì´ˆê¸° ì§‘ë‹¨ ìƒì„± ì¤‘...")
    initial_groups = create_spec_groups_from_clustering(top_specs, cfg)
    
    # ì´ˆê¸° ì§‘ë‹¨ í‰ê°€
    print(f"  âš–ï¸ ì´ˆê¸° ì§‘ë‹¨ í‰ê°€ ì¤‘... ({len(initial_groups)}ê°œ ê·¸ë£¹)")
    for i, group_specs in enumerate(initial_groups):
        print(f"    ê·¸ë£¹ {i+1}/{len(initial_groups)}: {len(group_specs)}ê°œ spec í‰ê°€ ì¤‘...")
        evaluated_group = evaluate_spec_group(
            group_specs, unified_judge,
            domain_profile, task_profile, -1, judges_dir
        )
        group_archive.add_group(evaluated_group)
    
    print(f"  ğŸ“Š ì´ˆê¸° ê·¸ë£¹: {len(group_archive.all_groups())}ê°œ")

    # ì´ˆê¸° ê·¸ë£¹ë“¤ë¡œ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if group_archive.all_groups():
        sorted_groups = sorted(group_archive.all_groups(), key=lambda g: g.get('group_score', 0), reverse=True)
        best_groups_history.extend(sorted_groups[:2])  # ìƒìœ„ 2ê°œ
        worst_groups_history.extend(sorted_groups[-2:])  # í•˜ìœ„ 2ê°œ

    # ì§„í™” ë£¨í”„
    history_path = os.path.join(out_dir, 'group_history.jsonl')
    
    with open(history_path, 'a', encoding='utf-8') as hf:
        for gen in range(cfg.generations):
            print(f"\nğŸš€ Generation {gen + 1}/{cfg.generations}")
            print("=" * 60)
            
            # ë¶€ëª¨ ê·¸ë£¹ ì„ íƒ
            all_groups = group_archive.all_groups()
            parent_groups = group_archive.sample_parent_groups(cfg.population_per_gen)
            print(f"  ğŸ§¬ ì„ íƒëœ ë¶€ëª¨ ê·¸ë£¹: {len(parent_groups)}ê°œ")
            
            # êµë°° ë° ë³€ì´
            print(f"  ğŸ”„ êµë°° ë° ë³€ì´ ì¤‘...")
            offspring_groups = group_crossover(parent_groups, cfg)
            mutated_groups = group_mutation(
                offspring_groups, client_gen, unified_judge, best_groups_history, worst_groups_history,
                constitution, domain_profile, task_profile, cfg, gen, generator_dir, domain_name, task_name,
                single_spec_pool=top_specs, rag_log_dir=rag_dir
            )
            
            print(f"  ğŸ“Š ìƒˆë¡œìš´ í›„ë³´ ê·¸ë£¹: {len(mutated_groups)}ê°œ")
            
            # í›„ë³´ ê·¸ë£¹ í‰ê°€
            print(f"  âš–ï¸ í›„ë³´ ê·¸ë£¹ í‰ê°€ ì¤‘...")
            evaluated_groups = []
            for i, group in enumerate(mutated_groups):
                print(f"    ê·¸ë£¹ {i+1}/{len(mutated_groups)}: {len(group['specs'])}ê°œ spec í‰ê°€ ì¤‘...")
                # evaluate_spec_groupì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ì‚¬ìš©
                evaluated_group = evaluate_spec_group(
                    group['specs'], unified_judge,
                    domain_profile, task_profile, gen, judges_dir
                )

                # ë©”íƒ€ë°ì´í„° ë³´ì¡´
                if 'meta' in group:
                    evaluated_group['meta'] = group['meta']
                evaluated_groups.append(evaluated_group)
            
            # ì ìˆ˜ ì •ê·œí™” ë° ê·¸ë£¹ Elo ê²½ìŸ
            if cfg.use_score_normalization:
                all_for_norm = group_archive.all_groups() + evaluated_groups
                # ê·¸ë£¹ ì ìˆ˜ ì •ê·œí™”
                if all_for_norm:
                    scores = [g.get('group_score', 0) for g in all_for_norm]
                    if len(set(scores)) > 1:
                        import statistics as st
                        mean_score = st.mean(scores)
                        std_score = st.pstdev(scores) or 1.0
                        for g in all_for_norm:
                            g['group_score_norm'] = (g.get('group_score', 0) - mean_score) / std_score

            # Pointwise í‰ê°€ë§Œ ì‚¬ìš© (Elo ê²½ìŸ ì œê±°)
            
            # ìµœê³ /ìµœì € ì„±ëŠ¥ ê·¸ë£¹ ê¸°ë¡ (good/bad examplesìš©)
            if evaluated_groups:
                sorted_by_score = sorted(evaluated_groups, key=lambda g: g.get('group_score', 0), reverse=True)
                best_groups_history.extend(sorted_by_score[:2])  # ìƒìœ„ 2ê°œ
                worst_groups_history.extend(sorted_by_score[-2:])  # í•˜ìœ„ 2ê°œ

                # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
                best_groups_history = best_groups_history[-5:]  # ìµœê·¼ 5ê°œ ìœ ì§€
                worst_groups_history = worst_groups_history[-5:]  # ìµœê·¼ 5ê°œ ìœ ì§€

            # Archiveì— ì¶”ê°€
            for group in evaluated_groups:
                group_archive.add_group(group)
            
            # í†µê³„ ë° ë¡œê·¸
            all_groups = group_archive.all_groups()
            best_group = all_groups[0] if all_groups else None
            best_score = best_group.get('group_score', 0) if best_group else 0
            
            record = {
                'generation': gen,
                'best_group_score': best_score,
                'archive_size': len(all_groups),
                'avg_group_size': sum(len(g.get('specs', [])) for g in all_groups) / len(all_groups) if all_groups else 0,
                'timestamp': time.time(),
            }
            hf.write(json.dumps(record, ensure_ascii=False) + "\n")
            hf.flush()
            
            # ìƒìœ„ ê·¸ë£¹ë“¤ ì €ì¥
            fname = os.path.join(best_dir, f"gen{gen:03d}_top_groups.md")
            with open(fname, 'w', encoding='utf-8') as wf:
                wf.write(f"# Generation {gen} - Top Groups\n\n")
                wf.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                wf.write(f"Archive size: {len(all_groups)}\n")
                wf.write(f"Best Group Score: {best_score:.1f}\n\n")
                
                for i, group in enumerate(all_groups[:10]):
                    wf.write(f"## Group #{i+1}\n\n")
                    wf.write(f"**ID:** {group['id']}\n")

                    wf.write(f"**Group Score:** {group.get('group_score', 0)}/100\n")
                    wf.write(f"**Scores:** Cohesion: {group.get('group_scores', {}).get('cohesion', 0)}/30, ")
                    wf.write(f"Coverage: {group.get('group_scores', {}).get('coverage', 0)}/25, ")
                    wf.write(f"Redundancy: {group.get('group_scores', {}).get('redundancy', 0)}/25, ")
                    wf.write(f"Practicality: {group.get('group_scores', {}).get('practicality', 0)}/20\n")
                    wf.write(f"**Group Size:** {len(group.get('specs', []))}\n\n")
                    
                    wf.write(f"**Specifications:**\n")
                    for j, spec in enumerate(group.get('specs', []), 1):
                        wf.write(f"{j}. {spec.get('text', '')}\n")
                    wf.write("\n" + "-"*60 + "\n\n")
            
            print(f"  ğŸ“Š Gen {gen}: best_score={best_score:.1f}, archive_size={len(all_groups)}")
    
    # ========== ìµœì¢… Spec ì¶”ì¶œ ë° ì €ì¥ (ver3ìš©) ==========
    print(f"\nğŸ† ìµœê³  ì ìˆ˜ ê·¸ë£¹ ì¶”ì¶œ ë° ì¤‘ë³µ ì œê±° ì¤‘...")
    
    all_final_groups = group_archive.all_groups()
    if all_final_groups:
        # ìµœê³  ì ìˆ˜ ê·¸ë£¹ ì¶”ì¶œ
        best_group = all_final_groups[0]
        best_specs = best_group.get('specs', [])
        
        print(f"  ğŸ“Š ìµœê³  ê·¸ë£¹: {len(best_specs)}ê°œ spec (ì ìˆ˜: {best_group.get('group_score', 0)}/100)")
        
        # LLMìœ¼ë¡œ ì¤‘ë³µ ì œê±°
        deduplicated_specs = deduplicate_specs_with_llm(
            best_specs, 
            client_gen,  # Generator client ì‚¬ìš©
            domain_profile, 
            task_profile
        )
        
        # JSON ì €ì¥
        final_spec_path = os.path.join(out_dir, 'final_spec.json')
        final_spec_data = {
            'domain': domain_name,
            'task': task_name,
            'generation': cfg.generations,
            'original_group_score': best_group.get('group_score', 0),
            'original_count': len(best_specs),
            'deduplicated_count': len(deduplicated_specs),
            'specifications': [
                {
                    'id': spec.get('id', ''),
                    'text': spec.get('text', ''),
                    'score': spec.get('score', 0),
                    'elo': spec.get('elo', 0)
                }
                for spec in deduplicated_specs
            ]
        }
        
        with open(final_spec_path, 'w', encoding='utf-8') as f:
            json.dump(final_spec_data, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… ìµœì¢… Spec ì €ì¥: {final_spec_path}")
        print(f"  ğŸ“ ìµœì¢… Spec ê°œìˆ˜: {len(deduplicated_specs)}ê°œ")
    
    print(f"\nâœ… ì§‘ë‹¨ ì§„í™” ì™„ë£Œ! ê²°ê³¼: {out_dir}")
    return group_archive


def run_domain_group_evolution(target_domain: str = "Legal_and_Regulatory", output_dir: str = "final_spec_ver1", generations: int = 5):
    """íŠ¹ì • ë„ë©”ì¸ì˜ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ ê·¸ë£¹ ì§„í™”ë¥¼ ì‹¤í–‰"""
    import os
    import sys
    from alpha_elo import Archive

    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "")
    openai_api_key = os.environ.get("OPENAI_API_KEY", "")

    # Client ìƒì„± (UnifiedGroupJudgeìš© - GPT-4o í•„ìˆ˜)
    if not openai_api_key:
        raise ValueError("âŒ Judgeìš© OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    from alpha_elo import OpenAIClientWrapper
    client = OpenAIClientWrapper(api_key=openai_api_key, model="gpt-4o")

    config = GroupEvolverConfig(
        anthropic_api_key=anthropic_api_key,
        openai_api_key=openai_api_key,
        generations=generations,  
        population_per_gen=6,  # ê·¸ë£¹ ìˆ˜ ì¦ê°€
        output_dir=output_dir,
        use_timestamp_suffix=False
    )

    # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ archive ìë™ ì°¾ê¸°
    def find_latest_archive(domain_name, task_name):
        """ë„ë©”ì¸/íƒœìŠ¤í¬ ì¡°í•©ì˜ ê°€ì¥ ìµœê·¼ archiveë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
        import os
        import re
        from datetime import datetime

        base_dir = "/Users/vashazza/Desktop/PJ/Fellowship/AlphaEvolve"

        # single_spec_result_ver1 í´ë” ì°¾ê¸°
        results_dir = "single_spec_result_ver1"
        
        # ë„ë©”ì¸/íƒœìŠ¤í¬ ì¡°í•©ì˜ archive ì°¾ê¸°
        domain_task_path = os.path.join(base_dir, results_dir, domain_name, task_name)
        archive_path = os.path.join(domain_task_path, "top100_archive.json")

        if os.path.exists(archive_path):
            return archive_path

        return None

    # ë„ë©”ì¸ ì„¤ì •
    domain_name = target_domain
    print(f"ğŸš€ ë„ë©”ì¸ '{domain_name}'ì˜ ê·¸ë£¹ ì§„í™” ì‹œì‘")

    # í•´ë‹¹ ë„ë©”ì¸ì˜ few_shot í´ë”ì—ì„œ ëª¨ë“  íƒœìŠ¤í¬ ì°¾ê¸°
    few_shot_folder = "few_shot_examples"
    safe_domain = target_domain
    domain_folder = os.path.join(few_shot_folder, safe_domain)
    ensure_dir(domain_folder)

    if not os.path.exists(domain_folder):
        print(f"âŒ ë„ë©”ì¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {domain_folder}")
        return

    task_files = [f for f in os.listdir(domain_folder) if f.endswith('.txt')]
    if not task_files:
        print(f"âš ï¸ {domain_folder}ì— íƒœìŠ¤í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“‹ ë°œê²¬ëœ íƒœìŠ¤í¬ë“¤: {task_files}")

    # UnifiedGroupJudge ìƒì„± (í•œ ë²ˆë§Œ)
    unified_judge = UnifiedGroupJudge(client)

    # ê° íƒœìŠ¤í¬ì— ëŒ€í•´ ê·¸ë£¹ ì§„í™” ì‹¤í–‰
    for task_file in task_files:
        task_name = task_file.replace('.txt', '').replace('_and_', ' & ').replace('_', ' ')
        # ì‹¤ì œ ì €ì¥ëœ í´ë”ëª… (ì–¸ë”ìŠ¤ì½”ì–´ ìœ ì§€)
        task_folder_name = task_file.replace('.txt', '')
        print(f"\nğŸ¯ íƒœìŠ¤í¬ '{task_name}' ê·¸ë£¹ ì§„í™” ì‹œì‘")

        # ê°€ì¥ ìµœê·¼ archive ì°¾ê¸° (í´ë”ëª…ìœ¼ë¡œ ê²€ìƒ‰)
        archive_path = find_latest_archive(domain_name, task_folder_name)

        if not archive_path:
            print(f"  âŒ {domain_name}/{task_folder_name} ì¡°í•©ì˜ archiveë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                archive_data = json.load(f)

            # ìƒìœ„ 80ê°œ specë§Œ ì‚¬ìš©
            test_specs = []
            for item in archive_data[:80]:
                test_specs.append({
                    'text': item['text'],
                    'id': item['id'],
                    'score': item['score'],
                    'elo': item['elo']
                })

        except Exception as e:
            print(f"  âŒ Archive ë¡œë“œ ì‹¤íŒ¨: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # Archive ìƒì„±
        test_archive = Archive()
        for spec in test_specs:
            test_archive.add(spec)

        # ë„ë©”ì¸/íƒœìŠ¤í¬ í”„ë¡œí•„ ì„¤ì •
        constitution = "Build secure, reliable, and user-friendly AI systems."
        domain_profile = f"### Domain: {domain_name.replace('_and_', ' & ').replace('_', ' ')}\n- Description: Domain-specific requirements"
        task_profile = f"### Task: {task_name}\n- Description: Task-specific requirements"

        try:
            group_archive = run_group_evolution_from_archive(
                test_archive, task_name, constitution, domain_profile, task_profile,
                config, unified_judge, domain_name=target_domain
            )
            print(f"  ğŸ† {task_name} ì§„í™” ì™„ë£Œ! ì´ {len(group_archive.all_groups())}ê°œ ê·¸ë£¹ ìƒì„±")

            # ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ìƒìœ„ ê·¸ë£¹ 1ê°œë§Œ)
            best_groups = group_archive.all_groups()[:1]
            if best_groups:
                group = best_groups[0]
                print(f"     ê·¸ë£¹ ì ìˆ˜: {group.get('group_score', 0)}/100")
                print(f"     ê·¸ë£¹ í¬ê¸°: {len(group.get('specs', []))}ê°œ spec")

        except Exception as e:
            print(f"  âŒ {task_name} ê·¸ë£¹ ì§„í™” ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    print(f"\nâœ… ë„ë©”ì¸ '{domain_name}'ì˜ ëª¨ë“  íƒœìŠ¤í¬ ê·¸ë£¹ ì§„í™” ì™„ë£Œ!")


def run_single_task_group_evolution(target_domain: str, target_task: str, output_dir: str = "final_spec_ver1", generations: int = 5):
    """íŠ¹ì • ë„ë©”ì¸ì˜ íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•´ì„œë§Œ ê·¸ë£¹ ì§„í™”ë¥¼ ì‹¤í–‰"""
    import os
    import sys
    from alpha_elo import Archive

    anthropic_api_key = os.environ.get("ANTHROPIC_API_KEY", "")
    openai_api_key = os.environ.get("OPENAI_API_KEY", "")

    # Client ìƒì„± (UnifiedGroupJudgeìš© - GPT-4o í•„ìˆ˜)
    if not openai_api_key:
        raise ValueError("âŒ Judgeìš© OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    from alpha_elo import OpenAIClientWrapper
    client = OpenAIClientWrapper(api_key=openai_api_key, model="gpt-4o")

    config = GroupEvolverConfig(
        anthropic_api_key=anthropic_api_key,
        openai_api_key=openai_api_key,
        generations=generations,
        population_per_gen=6,
        output_dir=output_dir,
        use_timestamp_suffix=False
    )

    # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ archive ìë™ ì°¾ê¸°
    def find_latest_archive(domain_name, task_name):
        """ë„ë©”ì¸/íƒœìŠ¤í¬ ì¡°í•©ì˜ ê°€ì¥ ìµœê·¼ archiveë¥¼ ì°¾ëŠ” í•¨ìˆ˜"""
        import os
        import re
        from datetime import datetime

        base_dir = "/Users/vashazza/Desktop/PJ/Fellowship/AlphaEvolve"

        # single_spec_result_ver1 í´ë” ì°¾ê¸°
        results_dir = "single_spec_result_ver1"
        
        # ë„ë©”ì¸/íƒœìŠ¤í¬ ì¡°í•©ì˜ archive ì°¾ê¸°
        domain_task_path = os.path.join(base_dir, results_dir, domain_name, task_name)
        archive_path = os.path.join(domain_task_path, "top100_archive.json")

        if os.path.exists(archive_path):
            return archive_path

        return None

    # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ ì„¤ì •
    domain_name = target_domain
    task_name = target_task
    # ì‹¤ì œ ì €ì¥ëœ í´ë”ëª… (ì–¸ë”ìŠ¤ì½”ì–´ ìœ ì§€)
    task_folder_name = target_task.replace(' & ', '_and_').replace(' ', '_')
    print(f"ğŸš€ ë‹¨ì¼ íƒœìŠ¤í¬ ê·¸ë£¹ ì§„í™”: {domain_name} / {task_name}")

    # ê°€ì¥ ìµœê·¼ archive ì°¾ê¸° (í´ë”ëª…ìœ¼ë¡œ ê²€ìƒ‰)
    archive_path = find_latest_archive(domain_name, task_folder_name)

    if not archive_path:
        print(f"âŒ {domain_name}/{task_name} ì¡°í•©ì˜ archiveë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    try:
        with open(archive_path, 'r', encoding='utf-8') as f:
            archive_data = json.load(f)

        # ìƒìœ„ 80ê°œ specë§Œ ì‚¬ìš©
        test_specs = []
        for item in archive_data[:80]:
            test_specs.append({
                'text': item['text'],
                'id': item['id'],
                'score': item['score'],
                'elo': item['elo']
            })

    except Exception as e:
        print(f"âŒ Archive ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # Archive ìƒì„±
    test_archive = Archive()
    for spec in test_specs:
        test_archive.add(spec)

    # ë„ë©”ì¸/íƒœìŠ¤í¬ í”„ë¡œí•„ ì„¤ì •
    constitution = "Build secure, reliable, and user-friendly AI systems."
    domain_profile = f"### Domain: {domain_name.replace('_and_', ' & ').replace('_', ' ')}\n- Description: Domain-specific requirements"
    task_profile = f"### Task: {task_name}\n- Description: Task-specific requirements"

    # UnifiedGroupJudge ìƒì„±
    unified_judge = UnifiedGroupJudge(client)

    try:
        group_archive = run_group_evolution_from_archive(
            test_archive, task_name, constitution, domain_profile, task_profile,
            config, unified_judge, domain_name=target_domain
        )
        print(f"ğŸ† {task_name} ì§„í™” ì™„ë£Œ! ì´ {len(group_archive.all_groups())}ê°œ ê·¸ë£¹ ìƒì„±")

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ìƒìœ„ ê·¸ë£¹ 1ê°œë§Œ)
        best_groups = group_archive.all_groups()[:1]
        if best_groups:
            group = best_groups[0]
            print(f"   ê·¸ë£¹ ì ìˆ˜: {group.get('group_score', 0)}/100")
            print(f"   ê·¸ë£¹ í¬ê¸°: {len(group.get('specs', []))}ê°œ spec")

    except Exception as e:
        print(f"âŒ {task_name} ê·¸ë£¹ ì§„í™” ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == '__main__':
    import sys
    import argparse

    parser = argparse.ArgumentParser(description='ê·¸ë£¹ ì§„í™” ì‹¤í–‰')
    parser.add_argument('domain', type=str, nargs='?', default="Legal_and_Regulatory",
                        help='íƒ€ê²Ÿ ë„ë©”ì¸ (ì˜ˆ: General, Healthcare_and_Medicine)')
    parser.add_argument('task', type=str, nargs='?', default=None,
                        help='íƒ€ê²Ÿ íƒœìŠ¤í¬ (ì„ íƒì‚¬í•­, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  íƒœìŠ¤í¬)')
    parser.add_argument('--output-dir', type=str, default="final_spec_ver1",
                        help='ì¶œë ¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸ê°’: final_spec_ver1)')
    parser.add_argument('--generations', type=int, default=5,
                        help='ì§„í™” ì„¸ëŒ€ ìˆ˜ (ê¸°ë³¸ê°’: 5)')
    
    args = parser.parse_args()

    if args.task:
        # ë„ë©”ì¸ê³¼ íƒœìŠ¤í¬ ë‘˜ ë‹¤ ì§€ì •ëœ ê²½ìš°
        print(f"ğŸ¯ Target: {args.domain} / {args.task}")
        print(f"ğŸ“ Output: {args.output_dir}")
        print(f"ğŸ”„ Generations: {args.generations}")
        run_single_task_group_evolution(args.domain, args.task, args.output_dir, args.generations)
    else:
        # ë„ë©”ì¸ë§Œ ì§€ì •ëœ ê²½ìš°
        print(f"ğŸ¯ Target Domain: {args.domain} (ëª¨ë“  íƒœìŠ¤í¬)")
        print(f"ğŸ“ Output: {args.output_dir}")
        print(f"ğŸ”„ Generations: {args.generations}")
        run_domain_group_evolution(args.domain, args.output_dir, args.generations)
